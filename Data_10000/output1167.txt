{"kind": "Listing", "data": {"after": "t1_j6xm8m9", "dist": 25, "modhash": null, "geo_filter": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "[D] What do people think about OpenAI not releasing its research but benefiting from others\u2019 research? Should google meta enforce its patents against them?", "mod_reason_by": null, "banned_by": null, "ups": 1, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "MachineLearning", "link_author": "NoScallion2450", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jck7qjx", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "Hyper1on", "num_comments": 165, "can_mod_post": false, "send_replies": true, "parent_id": "t1_jcbw6mx", "score": 1, "author_fullname": "t2_7c7fe", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "DM already does hoard their secrets, there are successful projects there which are not published. What they show you is what they decide needs to be public to get good PR.", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;DM already does hoard their secrets, there are successful projects there which are not published. What they show you is what they decide needs to be public to get good PR.&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11rtzv6", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/MachineLearning/comments/11rtzv6/d_what_do_people_think_about_openai_not_releasing/jck7qjx/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/MachineLearning/comments/11rtzv6/d_what_do_people_think_about_openai_not_releasing/", "name": "t1_jck7qjx", "author_flair_template_id": null, "subreddit_name_prefixed": "r/MachineLearning", "author_flair_text": null, "treatment_tags": [], "created": 1679058490.0, "created_utc": 1679058490.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/MachineLearning/comments/11rtzv6/d_what_do_people_think_about_openai_not_releasing/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "[D] Anyone else witnessing a panic inside NLP orgs of big tech companies?", "mod_reason_by": null, "banned_by": null, "ups": 2, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "MachineLearning", "link_author": "thrwsitaway4321", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jcbn3bz", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "Hyper1on", "num_comments": 481, "can_mod_post": false, "send_replies": true, "parent_id": "t1_jc8qapr", "score": 2, "author_fullname": "t2_7c7fe", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "Even big tech companies balk at spending in the high tens of millions for a single training run. And that's not counting the staff of over 100 very expensive devs and researchers it took to train and evaluate GPT-4. That said, big tech can absolutely catch up, they've just been asleep at the wheel for 3 years and OpenAI have a big lead.", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Even big tech companies balk at spending in the high tens of millions for a single training run. And that&amp;#39;s not counting the staff of over 100 very expensive devs and researchers it took to train and evaluate GPT-4. That said, big tech can absolutely catch up, they&amp;#39;ve just been asleep at the wheel for 3 years and OpenAI have a big lead.&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11rizyb", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/MachineLearning/comments/11rizyb/d_anyone_else_witnessing_a_panic_inside_nlp_orgs/jcbn3bz/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/MachineLearning/comments/11rizyb/d_anyone_else_witnessing_a_panic_inside_nlp_orgs/", "name": "t1_jcbn3bz", "author_flair_template_id": null, "subreddit_name_prefixed": "r/MachineLearning", "author_flair_text": null, "treatment_tags": [], "created": 1678903103.0, "created_utc": 1678903103.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/MachineLearning/comments/11rizyb/d_anyone_else_witnessing_a_panic_inside_nlp_orgs/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_3bzqh1", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "Morgan Stanley note on GPT-4/5 training demands, inference savings, Nvidia revenue, and LLM economics", "mod_reason_by": null, "banned_by": null, "ups": 1, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "mlscaling", "link_author": "gwern", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jc39dmz", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "Hyper1on", "num_comments": 7, "can_mod_post": false, "send_replies": true, "parent_id": "t1_jc2kfgy", "score": 1, "author_fullname": "t2_7c7fe", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "&gt; Within that mix, we would estimate that 90% of the AI revenue \u2014$9b\u2014comes from various forms of training, and about $1b from inference.\n\nThis is exactly opposite to the numbers I've seen quoted from Nvidia and others about inference vs training costs, e.g. this quote from 2019 https://www.hpcwire.com/2019/03/19/aws-upgrades-its-gpu-backed-ai-inference-platform/\n\n&gt; Inference \u201cis the big market,\u201d Nvidia CEO Jensen Huang stressed during the GPU maker\u2019s technology conference this week, with an estimated 80 to 90 percent of the cost of machine learning at scale devoted to AI inference.\n\nAlso, the fact that these analysts actually put in their report that they asked ChatGPT if it was using offload for inference and they apparently took that seriously makes me quite sceptical of their ability to draw good conclusions from the information they have.", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Within that mix, we would estimate that 90% of the AI revenue \u2014$9b\u2014comes from various forms of training, and about $1b from inference.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;This is exactly opposite to the numbers I&amp;#39;ve seen quoted from Nvidia and others about inference vs training costs, e.g. this quote from 2019 &lt;a href=\"https://www.hpcwire.com/2019/03/19/aws-upgrades-its-gpu-backed-ai-inference-platform/\"&gt;https://www.hpcwire.com/2019/03/19/aws-upgrades-its-gpu-backed-ai-inference-platform/&lt;/a&gt;&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Inference \u201cis the big market,\u201d Nvidia CEO Jensen Huang stressed during the GPU maker\u2019s technology conference this week, with an estimated 80 to 90 percent of the cost of machine learning at scale devoted to AI inference.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Also, the fact that these analysts actually put in their report that they asked ChatGPT if it was using offload for inference and they apparently took that seriously makes me quite sceptical of their ability to draw good conclusions from the information they have.&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11pnhpf", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/mlscaling/comments/11pnhpf/morgan_stanley_note_on_gpt45_training_demands/jc39dmz/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/mlscaling/comments/11pnhpf/morgan_stanley_note_on_gpt45_training_demands/", "name": "t1_jc39dmz", "author_flair_template_id": null, "subreddit_name_prefixed": "r/mlscaling", "author_flair_text": null, "treatment_tags": [], "created": 1678733361.0, "created_utc": 1678733361.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://twitter.com/davidtayar5/status/1627690520456691712"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2sfn3", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "Report details how Big Tech is leaning on EU not to regulate general purpose AIs", "mod_reason_by": null, "banned_by": null, "ups": 3, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "neoliberal", "link_author": "tubbsmackinze", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jae9x0x", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "Hyper1on", "num_comments": 23, "can_mod_post": false, "send_replies": true, "parent_id": "t1_jae5msp", "score": 3, "author_fullname": "t2_7c7fe", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "I just described how general-purpose AI may create harms that non-general-purpose AI doesn't necessarily pose. This originally started with a discussion about the EU AI Act, but this act is not regulation against AIs writing code, writing essays, or even acting as propaganda bots on Twitter. I don't think these things should be crimes, but if you're deploying a system that can do those things, then it seems reasonable to expect some level of scrutiny or risk assessment so that governments and users can have some idea of what the capabilities and potential harms of any particular general purpose model are.", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I just described how general-purpose AI may create harms that non-general-purpose AI doesn&amp;#39;t necessarily pose. This originally started with a discussion about the EU AI Act, but this act is not regulation against AIs writing code, writing essays, or even acting as propaganda bots on Twitter. I don&amp;#39;t think these things should be crimes, but if you&amp;#39;re deploying a system that can do those things, then it seems reasonable to expect some level of scrutiny or risk assessment so that governments and users can have some idea of what the capabilities and potential harms of any particular general purpose model are.&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11ed92r", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/neoliberal/comments/11ed92r/report_details_how_big_tech_is_leaning_on_eu_not/jae9x0x/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/neoliberal/comments/11ed92r/report_details_how_big_tech_is_leaning_on_eu_not/", "name": "t1_jae9x0x", "author_flair_template_id": null, "subreddit_name_prefixed": "r/neoliberal", "author_flair_text": null, "treatment_tags": [], "created": 1677616133.0, "created_utc": 1677616133.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://techcrunch.com/2023/02/23/eu-ai-act-lobbying-report/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2sfn3", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "Report details how Big Tech is leaning on EU not to regulate general purpose AIs", "mod_reason_by": null, "banned_by": null, "ups": 6, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "neoliberal", "link_author": "tubbsmackinze", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jae4jdn", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": false, "author": "Hyper1on", "num_comments": 23, "can_mod_post": false, "send_replies": true, "parent_id": "t1_jadkz51", "score": 6, "author_fullname": "t2_7c7fe", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "Because general purpose models are general, it means that even if you use a general purpose AI for some specific task, such as search, summarization, as a customer service bot, etc, then the model will have the ability to do many things if prompted correctly, which a non-general model would be unable to. For example, the customer service bot may be connected to an API to a database, and if adversarially prompted well enough could expose information the company may not want public (obviously this is a problem with the API design, but this kind of capability from an AI model is often unanticipated). We may also see emergent capabilities, where above a certain scale models become capable of unknown new abilities and new types of harms. One example of this might be Bing Chat, which in contrast to a normal chatbot which is specialised only for chat, is also capable of writing code (including computer viruses, in theory), writing plagiarised essays, or other potentially harmful language tasks.\n\nIn general I think there are sufficient risks unique to general purpose models (not just the use of general purpose models for normally harmful stuff) to be cautious. This paper is a good overview IMO: https://arxiv.org/abs/2202.07785", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Because general purpose models are general, it means that even if you use a general purpose AI for some specific task, such as search, summarization, as a customer service bot, etc, then the model will have the ability to do many things if prompted correctly, which a non-general model would be unable to. For example, the customer service bot may be connected to an API to a database, and if adversarially prompted well enough could expose information the company may not want public (obviously this is a problem with the API design, but this kind of capability from an AI model is often unanticipated). We may also see emergent capabilities, where above a certain scale models become capable of unknown new abilities and new types of harms. One example of this might be Bing Chat, which in contrast to a normal chatbot which is specialised only for chat, is also capable of writing code (including computer viruses, in theory), writing plagiarised essays, or other potentially harmful language tasks.&lt;/p&gt;\n\n&lt;p&gt;In general I think there are sufficient risks unique to general purpose models (not just the use of general purpose models for normally harmful stuff) to be cautious. This paper is a good overview IMO: &lt;a href=\"https://arxiv.org/abs/2202.07785\"&gt;https://arxiv.org/abs/2202.07785&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11ed92r", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/neoliberal/comments/11ed92r/report_details_how_big_tech_is_leaning_on_eu_not/jae4jdn/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/neoliberal/comments/11ed92r/report_details_how_big_tech_is_leaning_on_eu_not/", "name": "t1_jae4jdn", "author_flair_template_id": null, "subreddit_name_prefixed": "r/neoliberal", "author_flair_text": null, "treatment_tags": [], "created": 1677614087.0, "created_utc": 1677614087.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://techcrunch.com/2023/02/23/eu-ai-act-lobbying-report/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2sfn3", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "Report details how Big Tech is leaning on EU not to regulate general purpose AIs", "mod_reason_by": null, "banned_by": null, "ups": 3, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "neoliberal", "link_author": "tubbsmackinze", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jadjjud", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "Hyper1on", "num_comments": 23, "can_mod_post": false, "send_replies": true, "parent_id": "t1_jadeeh1", "score": 3, "author_fullname": "t2_7c7fe", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "I don't see why not? General purpose AI has the potential to create just as much harm, through both the same vectors as more specialised models, and also through some potential new ones, in my opinion as a researcher and developer of large AI models. PygmalionAI is just a finetune of an existing (relatively small) open source model, it's not from scratch, and it's perfectly correct to say that almost no small companies have the expertise to develop their own models from scratch. It's well known in the industry that there are only a handful of serious players able to make general purpose AI models, and people have been talking about this consolidation of power and knowledge quite a bit in the media over the past few years. However, I agree that there are and will be open-source general models (they're just likely to be worse than ChatGPT), and that the lobbying tactics here are not really that concerning.", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I don&amp;#39;t see why not? General purpose AI has the potential to create just as much harm, through both the same vectors as more specialised models, and also through some potential new ones, in my opinion as a researcher and developer of large AI models. PygmalionAI is just a finetune of an existing (relatively small) open source model, it&amp;#39;s not from scratch, and it&amp;#39;s perfectly correct to say that almost no small companies have the expertise to develop their own models from scratch. It&amp;#39;s well known in the industry that there are only a handful of serious players able to make general purpose AI models, and people have been talking about this consolidation of power and knowledge quite a bit in the media over the past few years. However, I agree that there are and will be open-source general models (they&amp;#39;re just likely to be worse than ChatGPT), and that the lobbying tactics here are not really that concerning.&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11ed92r", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/neoliberal/comments/11ed92r/report_details_how_big_tech_is_leaning_on_eu_not/jadjjud/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/neoliberal/comments/11ed92r/report_details_how_big_tech_is_leaning_on_eu_not/", "name": "t1_jadjjud", "author_flair_template_id": null, "subreddit_name_prefixed": "r/neoliberal", "author_flair_text": null, "treatment_tags": [], "created": 1677606168.0, "created_utc": 1677606168.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://techcrunch.com/2023/02/23/eu-ai-act-lobbying-report/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2qnvz", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "WSJ News Exclusive | Lab Leak Most Likely Origin of Covid-19 Pandemic, U.S. Agency Now Says", "mod_reason_by": null, "banned_by": null, "ups": 4, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "Destiny", "link_author": "Kind-Station9752", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "ja4wx71", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": false, "author": "Hyper1on", "num_comments": 72, "can_mod_post": false, "send_replies": true, "parent_id": "t1_ja4jcw3", "score": 4, "author_fullname": "t2_7c7fe", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "They're implying they are informed by classified information I believe, which certainly makes it at least plausible that they have information we don't which would lean towards the lab leak theory. The papers make a convincing argument but there could be other information the authors don't know.", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;They&amp;#39;re implying they are informed by classified information I believe, which certainly makes it at least plausible that they have information we don&amp;#39;t which would lean towards the lab leak theory. The papers make a convincing argument but there could be other information the authors don&amp;#39;t know.&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11chs7j", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/Destiny/comments/11chs7j/wsj_news_exclusive_lab_leak_most_likely_origin_of/ja4wx71/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/Destiny/comments/11chs7j/wsj_news_exclusive_lab_leak_most_likely_origin_of/", "name": "t1_ja4wx71", "author_flair_template_id": null, "subreddit_name_prefixed": "r/Destiny", "author_flair_text": null, "treatment_tags": [], "created": 1677447431.0, "created_utc": 1677447431.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.wsj.com/articles/covid-origin-china-lab-leak-807b7b0a?mod=Searchresults_pos1&amp;page=1"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "[D] To the ML researchers and practitioners here, do you worry about AI safety/alignment of the type Eliezer Yudkowsky describes?", "mod_reason_by": null, "banned_by": null, "ups": 3, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "MachineLearning", "link_author": "SchmidhuberDidIt", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "j9y3vz1", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "Hyper1on", "num_comments": 171, "can_mod_post": false, "send_replies": true, "parent_id": "t1_j9x01v0", "score": 3, "author_fullname": "t2_7c7fe", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "I mean, I don't see how you get a plausible explanation of BingGPT from underfitting either. As you say, models are underfit on some types of data, but I think the key here is the finetuning procedure, either normal supervised, or RLHF, which is optimising for a particular type of dialogue data in which the model is asked to act as an \"Assistant\" to a human user. \n\nPart of the reason I suspect my explanation is right is that ChatGPT and BingGPT were almost certainly finetuned on large amounts of dialogue data, collected from interactions with users, and yet most of the failure modes of BingGPT that made the media are not stuff like \"we asked it to solve this complex reasoning problem and it failed horribly\", they are instead coming from prompts which are very much in distribution for dialogue data, such as asking the model what it thinks about X, or asking the model to pretend it is Y and you would expect the model to have seen dialogues which start similarly before. I find underfitting on this data to be quite unlikely as an explanation.", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I mean, I don&amp;#39;t see how you get a plausible explanation of BingGPT from underfitting either. As you say, models are underfit on some types of data, but I think the key here is the finetuning procedure, either normal supervised, or RLHF, which is optimising for a particular type of dialogue data in which the model is asked to act as an &amp;quot;Assistant&amp;quot; to a human user. &lt;/p&gt;\n\n&lt;p&gt;Part of the reason I suspect my explanation is right is that ChatGPT and BingGPT were almost certainly finetuned on large amounts of dialogue data, collected from interactions with users, and yet most of the failure modes of BingGPT that made the media are not stuff like &amp;quot;we asked it to solve this complex reasoning problem and it failed horribly&amp;quot;, they are instead coming from prompts which are very much in distribution for dialogue data, such as asking the model what it thinks about X, or asking the model to pretend it is Y and you would expect the model to have seen dialogues which start similarly before. I find underfitting on this data to be quite unlikely as an explanation.&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11ada91", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/MachineLearning/comments/11ada91/d_to_the_ml_researchers_and_practitioners_here_do/j9y3vz1/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/MachineLearning/comments/11ada91/d_to_the_ml_researchers_and_practitioners_here_do/", "name": "t1_j9y3vz1", "author_flair_template_id": null, "subreddit_name_prefixed": "r/MachineLearning", "author_flair_text": null, "treatment_tags": [], "created": 1677327067.0, "created_utc": 1677327067.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/MachineLearning/comments/11ada91/d_to_the_ml_researchers_and_practitioners_here_do/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "[D] To the ML researchers and practitioners here, do you worry about AI safety/alignment of the type Eliezer Yudkowsky describes?", "mod_reason_by": null, "banned_by": null, "ups": 3, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "MachineLearning", "link_author": "SchmidhuberDidIt", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "j9wbysn", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "Hyper1on", "num_comments": 171, "can_mod_post": false, "send_replies": true, "parent_id": "t1_j9w6m9c", "score": 3, "author_fullname": "t2_7c7fe", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "Well, the obvious optimisation shortcoming is overfitting. We cannot distinguish this rigorously without access to model weights, but we also have a good idea what overfitting looks like in both pretraining and RL finetuning (in both cases it tends to result in common repeated text strings and a strong lack of diversity in output, a sort of pseudo mode collapse). We can test this by giving Bing GPT the same question multiple times and observing if it has a strong bias towards particular completions -- having played with it a bit I don't think this is really true for the original version, before Microsoft limited it in response to criticism a few days ago. \n\nMeanwhile, the alternative hypothesis I raised seems very plausible and fits logically with prior work on emergent capabilities of LLMs (https://arxiv.org/abs/2206.07682), since it seems only natural to expect that when you optimise a powerful system for an objective sufficiently, it will learn instrumental behaviours which help it minimise that objective, potentially up to and including appearing to simulate various \"personalities\" and other strange outputs.\n\nPersonally, as a researcher who works on RL finetuned large language models and has spent time playing with many of these models, my intuition is that Bing GPT is not RL finetuned at all but is just pretrained and finetuned on dialogue data, and the behaviour we see is just fairly likely to arise by default, given Bing GPT's particular model architecture and datasets (and prompting interaction with the Bing Search API).", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Well, the obvious optimisation shortcoming is overfitting. We cannot distinguish this rigorously without access to model weights, but we also have a good idea what overfitting looks like in both pretraining and RL finetuning (in both cases it tends to result in common repeated text strings and a strong lack of diversity in output, a sort of pseudo mode collapse). We can test this by giving Bing GPT the same question multiple times and observing if it has a strong bias towards particular completions -- having played with it a bit I don&amp;#39;t think this is really true for the original version, before Microsoft limited it in response to criticism a few days ago. &lt;/p&gt;\n\n&lt;p&gt;Meanwhile, the alternative hypothesis I raised seems very plausible and fits logically with prior work on emergent capabilities of LLMs (&lt;a href=\"https://arxiv.org/abs/2206.07682\"&gt;https://arxiv.org/abs/2206.07682&lt;/a&gt;), since it seems only natural to expect that when you optimise a powerful system for an objective sufficiently, it will learn instrumental behaviours which help it minimise that objective, potentially up to and including appearing to simulate various &amp;quot;personalities&amp;quot; and other strange outputs.&lt;/p&gt;\n\n&lt;p&gt;Personally, as a researcher who works on RL finetuned large language models and has spent time playing with many of these models, my intuition is that Bing GPT is not RL finetuned at all but is just pretrained and finetuned on dialogue data, and the behaviour we see is just fairly likely to arise by default, given Bing GPT&amp;#39;s particular model architecture and datasets (and prompting interaction with the Bing Search API).&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11ada91", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/MachineLearning/comments/11ada91/d_to_the_ml_researchers_and_practitioners_here_do/j9wbysn/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/MachineLearning/comments/11ada91/d_to_the_ml_researchers_and_practitioners_here_do/", "name": "t1_j9wbysn", "author_flair_template_id": null, "subreddit_name_prefixed": "r/MachineLearning", "author_flair_text": null, "treatment_tags": [], "created": 1677286177.0, "created_utc": 1677286177.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/MachineLearning/comments/11ada91/d_to_the_ml_researchers_and_practitioners_here_do/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "[D] To the ML researchers and practitioners here, do you worry about AI safety/alignment of the type Eliezer Yudkowsky describes?", "mod_reason_by": null, "banned_by": null, "ups": 3, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "MachineLearning", "link_author": "SchmidhuberDidIt", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "j9w5k2x", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "Hyper1on", "num_comments": 171, "can_mod_post": false, "send_replies": true, "parent_id": "t1_j9vzhgy", "score": 3, "author_fullname": "t2_7c7fe", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "I mean, it seems like the obvious explanation? That the model's behaviour is incentivised by its training objective. It also seems very plausible: we know that language models at large scale (even if not RL finetuned) exhibit a wide variety of emergent behaviours which you might not guess are motivated by next token prediction, but evidently are instrumental to reducing the loss. This is not necessarily overfitting: the argument is simply that certain behaviour unanticipated by the researchers is incentivised when you minimise the loss function. Arguably, this is a case of goal misgeneralisation: https://arxiv.org/abs/2105.14111", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I mean, it seems like the obvious explanation? That the model&amp;#39;s behaviour is incentivised by its training objective. It also seems very plausible: we know that language models at large scale (even if not RL finetuned) exhibit a wide variety of emergent behaviours which you might not guess are motivated by next token prediction, but evidently are instrumental to reducing the loss. This is not necessarily overfitting: the argument is simply that certain behaviour unanticipated by the researchers is incentivised when you minimise the loss function. Arguably, this is a case of goal misgeneralisation: &lt;a href=\"https://arxiv.org/abs/2105.14111\"&gt;https://arxiv.org/abs/2105.14111&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11ada91", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/MachineLearning/comments/11ada91/d_to_the_ml_researchers_and_practitioners_here_do/j9w5k2x/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/MachineLearning/comments/11ada91/d_to_the_ml_researchers_and_practitioners_here_do/", "name": "t1_j9w5k2x", "author_flair_template_id": null, "subreddit_name_prefixed": "r/MachineLearning", "author_flair_text": null, "treatment_tags": [], "created": 1677283331.0, "created_utc": 1677283331.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/MachineLearning/comments/11ada91/d_to_the_ml_researchers_and_practitioners_here_do/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "[D] Yann LeCun's Hot Take about programming languages for ML", "mod_reason_by": null, "banned_by": null, "ups": 1, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "MachineLearning", "link_author": "Marcapiel", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "j9vw853", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "Hyper1on", "num_comments": 276, "can_mod_post": false, "send_replies": true, "parent_id": "t1_j9otokt", "score": 1, "author_fullname": "t2_7c7fe", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "ChatGPT is a highly iterated on RLHF finetune of a supervised finetune on top of GPT-3 175B, served to a hundred million users efficiently with very low latency. All the components there except the basic idea of transformers came from OpenAI, and most LLM researchers I know would agree they are ahead of Google in LLM research.", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;ChatGPT is a highly iterated on RLHF finetune of a supervised finetune on top of GPT-3 175B, served to a hundred million users efficiently with very low latency. All the components there except the basic idea of transformers came from OpenAI, and most LLM researchers I know would agree they are ahead of Google in LLM research.&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_119r6m0", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/MachineLearning/comments/119r6m0/d_yann_lecuns_hot_take_about_programming/j9vw853/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/MachineLearning/comments/119r6m0/d_yann_lecuns_hot_take_about_programming/", "name": "t1_j9vw853", "author_flair_template_id": null, "subreddit_name_prefixed": "r/MachineLearning", "author_flair_text": null, "treatment_tags": [], "created": 1677279310.0, "created_utc": 1677279310.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/MachineLearning/comments/119r6m0/d_yann_lecuns_hot_take_about_programming/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "[D] To the ML researchers and practitioners here, do you worry about AI safety/alignment of the type Eliezer Yudkowsky describes?", "mod_reason_by": null, "banned_by": null, "ups": 4, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "MachineLearning", "link_author": "SchmidhuberDidIt", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "j9vuyzm", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": false, "author": "Hyper1on", "num_comments": 171, "can_mod_post": false, "send_replies": true, "parent_id": "t1_j9ux0jn", "score": 4, "author_fullname": "t2_7c7fe", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "The hypothesis is precisely that the failure mode of Bing Chat comes from it being too strong, not too weak. That is, if prompted even in quite vague ways it can exhibit instrumentally convergent behaviour like threatening you, even though this was obviously not the designer's objective, and this behaviour occurs as a byproduct of being highly optimised to predict the next word (or an RL finetuning training objective). This is obviously not possible with, say, GPT-2, because GPT-2 does not have enough capacity or data thrown at it to do that.", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The hypothesis is precisely that the failure mode of Bing Chat comes from it being too strong, not too weak. That is, if prompted even in quite vague ways it can exhibit instrumentally convergent behaviour like threatening you, even though this was obviously not the designer&amp;#39;s objective, and this behaviour occurs as a byproduct of being highly optimised to predict the next word (or an RL finetuning training objective). This is obviously not possible with, say, GPT-2, because GPT-2 does not have enough capacity or data thrown at it to do that.&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11ada91", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/MachineLearning/comments/11ada91/d_to_the_ml_researchers_and_practitioners_here_do/j9vuyzm/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/MachineLearning/comments/11ada91/d_to_the_ml_researchers_and_practitioners_here_do/", "name": "t1_j9vuyzm", "author_flair_template_id": null, "subreddit_name_prefixed": "r/MachineLearning", "author_flair_text": null, "treatment_tags": [], "created": 1677278788.0, "created_utc": 1677278788.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/MachineLearning/comments/11ada91/d_to_the_ml_researchers_and_practitioners_here_do/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "[D] To the ML researchers and practitioners here, do you worry about AI safety/alignment of the type Eliezer Yudkowsky describes?", "mod_reason_by": null, "banned_by": null, "ups": 2, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "MachineLearning", "link_author": "SchmidhuberDidIt", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "j9vudgi", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "Hyper1on", "num_comments": 171, "can_mod_post": false, "send_replies": true, "parent_id": "t1_j9rzhcc", "score": 2, "author_fullname": "t2_7c7fe", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "Just wanted to point out that even if we restrict ourselves purely to an agent that can only interact with the world through the internet, code, and natural language, that does not address the core AI alignment arguments of instrumental convergence etc being dangerous.", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Just wanted to point out that even if we restrict ourselves purely to an agent that can only interact with the world through the internet, code, and natural language, that does not address the core AI alignment arguments of instrumental convergence etc being dangerous.&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11ada91", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/MachineLearning/comments/11ada91/d_to_the_ml_researchers_and_practitioners_here_do/j9vudgi/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/MachineLearning/comments/11ada91/d_to_the_ml_researchers_and_practitioners_here_do/", "name": "t1_j9vudgi", "author_flair_template_id": null, "subreddit_name_prefixed": "r/MachineLearning", "author_flair_text": null, "treatment_tags": [], "created": 1677278539.0, "created_utc": 1677278539.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/MachineLearning/comments/11ada91/d_to_the_ml_researchers_and_practitioners_here_do/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2sfn3", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "UK worker and student visa issuance hit a record 753,428 in 2022, up 86% from 2019", "mod_reason_by": null, "banned_by": null, "ups": 51, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "neoliberal", "link_author": "ldn6", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "j9tsrst", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": false, "author": "Hyper1on", "num_comments": 39, "can_mod_post": false, "send_replies": true, "parent_id": "t3_11aqs7j", "score": 51, "author_fullname": "t2_7c7fe", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "Very nice to see. I hear from a lot of tech startup people in London that the UK is one of the easiest possible countries to get international talent to, since visa applications get granted very quickly for skilled workers.", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Very nice to see. I hear from a lot of tech startup people in London that the UK is one of the easiest possible countries to get international talent to, since visa applications get granted very quickly for skilled workers.&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11aqs7j", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/neoliberal/comments/11aqs7j/uk_worker_and_student_visa_issuance_hit_a_record/j9tsrst/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/neoliberal/comments/11aqs7j/uk_worker_and_student_visa_issuance_hit_a_record/", "name": "t1_j9tsrst", "author_flair_template_id": null, "subreddit_name_prefixed": "r/neoliberal", "author_flair_text": null, "treatment_tags": [], "created": 1677249921.0, "created_utc": 1677249921.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.gov.uk/government/statistics/immigration-system-statistics-year-ending-december-2022/summary-of-latest-statistics"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2sfn3", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "What esoteric/niche issue do you have strong views on that most people on this sub likely don\u2019t think about?", "mod_reason_by": null, "banned_by": null, "ups": 7, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "neoliberal", "link_author": "LukeBabbitt", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "j9as2rd", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": false, "author": "Hyper1on", "num_comments": 608, "can_mod_post": false, "send_replies": true, "parent_id": "t1_j9amqsg", "score": 7, "author_fullname": "t2_7c7fe", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "I'll note that this is what you would expect if the social stratum was aligned with \"merit\", as well as potentially if it was not (if merit is defined in a way to keep actually deserving people who have bad life circumstances out of institutions).\n\nSo the question really becomes, why is merit worse than alternatives as an influence on the social stratum?", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ll note that this is what you would expect if the social stratum was aligned with &amp;quot;merit&amp;quot;, as well as potentially if it was not (if merit is defined in a way to keep actually deserving people who have bad life circumstances out of institutions).&lt;/p&gt;\n\n&lt;p&gt;So the question really becomes, why is merit worse than alternatives as an influence on the social stratum?&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_1176805", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/neoliberal/comments/1176805/what_esotericniche_issue_do_you_have_strong_views/j9as2rd/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/neoliberal/comments/1176805/what_esotericniche_issue_do_you_have_strong_views/", "name": "t1_j9as2rd", "author_flair_template_id": null, "subreddit_name_prefixed": "r/neoliberal", "author_flair_text": null, "treatment_tags": [], "created": 1676908049.0, "created_utc": 1676908049.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/neoliberal/comments/1176805/what_esotericniche_issue_do_you_have_strong_views/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2sfn3", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "What esoteric/niche issue do you have strong views on that most people on this sub likely don\u2019t think about?", "mod_reason_by": null, "banned_by": null, "ups": 5, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "neoliberal", "link_author": "LukeBabbitt", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "j9arq53", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": false, "author": "Hyper1on", "num_comments": 608, "can_mod_post": false, "send_replies": true, "parent_id": "t1_j9aq90i", "score": 5, "author_fullname": "t2_7c7fe", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "I think helmet laws are generally bad because they don't fit every situation.\nI do most of my cycling in cities with good bike infra, and rarely need to cycle on roads -- as a result I wear a helmet only when I know I will be in close contact with traffic (so the risk is higher).", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think helmet laws are generally bad because they don&amp;#39;t fit every situation.\nI do most of my cycling in cities with good bike infra, and rarely need to cycle on roads -- as a result I wear a helmet only when I know I will be in close contact with traffic (so the risk is higher).&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_1176805", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/neoliberal/comments/1176805/what_esotericniche_issue_do_you_have_strong_views/j9arq53/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/neoliberal/comments/1176805/what_esotericniche_issue_do_you_have_strong_views/", "name": "t1_j9arq53", "author_flair_template_id": null, "subreddit_name_prefixed": "r/neoliberal", "author_flair_text": null, "treatment_tags": [], "created": 1676907907.0, "created_utc": 1676907907.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/neoliberal/comments/1176805/what_esotericniche_issue_do_you_have_strong_views/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2sfn3", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "According to Pew, 65% of democrats believe that the government should censor misinformation on the internet", "mod_reason_by": null, "banned_by": null, "ups": 0, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "neoliberal", "link_author": "flenserdc", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "j928t92", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "Hyper1on", "num_comments": 503, "can_mod_post": false, "send_replies": true, "parent_id": "t1_j91dngb", "score": 0, "author_fullname": "t2_7c7fe", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "TBH this sounds fine to me, because it appears like a system with Byzantine complexity that was set up to minimise the overall effect being frequent removals of misinformation that isn't actually misinformation (i.e. false positives). And I think that's broadly a good approach.", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;TBH this sounds fine to me, because it appears like a system with Byzantine complexity that was set up to minimise the overall effect being frequent removals of misinformation that isn&amp;#39;t actually misinformation (i.e. false positives). And I think that&amp;#39;s broadly a good approach.&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_115fjnl", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/neoliberal/comments/115fjnl/according_to_pew_65_of_democrats_believe_that_the/j928t92/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/neoliberal/comments/115fjnl/according_to_pew_65_of_democrats_believe_that_the/", "name": "t1_j928t92", "author_flair_template_id": null, "subreddit_name_prefixed": "r/neoliberal", "author_flair_text": null, "treatment_tags": [], "created": 1676743325.0, "created_utc": 1676743325.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/neoliberal/comments/115fjnl/according_to_pew_65_of_democrats_believe_that_the/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "[D] Bing: \u201cI will not harm you unless you harm me first\u201d", "mod_reason_by": null, "banned_by": null, "ups": 1, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "MachineLearning", "link_author": "blabboy", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "j8xflel", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "Hyper1on", "num_comments": 245, "can_mod_post": false, "send_replies": true, "parent_id": "t1_j8v1yce", "score": 1, "author_fullname": "t2_7c7fe", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "I've tried character.ai, plain GPT-3, and ChatGPT, and none of them come close to this.", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I&amp;#39;ve tried character.ai, plain GPT-3, and ChatGPT, and none of them come close to this.&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_113m3ea", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/MachineLearning/comments/113m3ea/d_bing_i_will_not_harm_you_unless_you_harm_me/j8xflel/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/MachineLearning/comments/113m3ea/d_bing_i_will_not_harm_you_unless_you_harm_me/", "name": "t1_j8xflel", "author_flair_template_id": null, "subreddit_name_prefixed": "r/MachineLearning", "author_flair_text": null, "treatment_tags": [], "created": 1676654222.0, "created_utc": 1676654222.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/MachineLearning/comments/113m3ea/d_bing_i_will_not_harm_you_unless_you_harm_me/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "[D] Bing: \u201cI will not harm you unless you harm me first\u201d", "mod_reason_by": null, "banned_by": null, "ups": 5, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "MachineLearning", "link_author": "blabboy", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "j8sjwgq", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": false, "author": "Hyper1on", "num_comments": 245, "can_mod_post": false, "send_replies": true, "parent_id": "t1_j8rcgy4", "score": 5, "author_fullname": "t2_7c7fe", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "I think it's obviously different to GPT3.5. We have never seen anything like the coherent personality of Bing in any GPT-3 model. Not to mention that one-shot essay writing capability of multiple thousand words was never a GPT3.5 thing: https://twitter.com/emollick/status/1625701942574960646", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;I think it&amp;#39;s obviously different to GPT3.5. We have never seen anything like the coherent personality of Bing in any GPT-3 model. Not to mention that one-shot essay writing capability of multiple thousand words was never a GPT3.5 thing: &lt;a href=\"https://twitter.com/emollick/status/1625701942574960646\"&gt;https://twitter.com/emollick/status/1625701942574960646&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_113m3ea", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/MachineLearning/comments/113m3ea/d_bing_i_will_not_harm_you_unless_you_harm_me/j8sjwgq/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/MachineLearning/comments/113m3ea/d_bing_i_will_not_harm_you_unless_you_harm_me/", "name": "t1_j8sjwgq", "author_flair_template_id": null, "subreddit_name_prefixed": "r/MachineLearning", "author_flair_text": null, "treatment_tags": [], "created": 1676568595.0, "created_utc": 1676568595.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/MachineLearning/comments/113m3ea/d_bing_i_will_not_harm_you_unless_you_harm_me/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_38jf0", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "ChatGPT reveals it's plans for the future of humanity", "mod_reason_by": null, "banned_by": null, "ups": 1, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "LivestreamFail", "link_author": "futoohell", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "j84hnuh", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "Hyper1on", "num_comments": 31, "can_mod_post": false, "send_replies": true, "parent_id": "t1_j83h1o1", "score": 1, "author_fullname": "t2_7c7fe", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "/r/SubSimulatorGPT2/", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;&lt;a href=\"/r/SubSimulatorGPT2/\"&gt;/r/SubSimulatorGPT2/&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_10zbkbc", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/LivestreamFail/comments/10zbkbc/chatgpt_reveals_its_plans_for_the_future_of/j84hnuh/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/LivestreamFail/comments/10zbkbc/chatgpt_reveals_its_plans_for_the_future_of/", "name": "t1_j84hnuh", "author_flair_template_id": null, "subreddit_name_prefixed": "r/LivestreamFail", "author_flair_text": null, "treatment_tags": [], "created": 1676130356.0, "created_utc": 1676130356.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://youtube.com/clip/UgkxDnil0Kk0NQzwThy7lpbb1h_zBm5OWE_Y"}}, {"kind": "t1", "data": {"subreddit_id": "t5_38jf0", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "London IRL streamer meets The King of England", "mod_reason_by": null, "banned_by": null, "ups": 1, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "LivestreamFail", "link_author": "TheBatteryChicken", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "j7s52x6", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "Hyper1on", "num_comments": 72, "can_mod_post": false, "send_replies": true, "parent_id": "t1_j7qoisd", "score": 1, "author_fullname": "t2_7c7fe", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "Yes, but is his favourite blend Earl Gray? English breakfast?", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes, but is his favourite blend Earl Gray? English breakfast?&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_10wuxg6", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/LivestreamFail/comments/10wuxg6/london_irl_streamer_meets_the_king_of_england/j7s52x6/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/LivestreamFail/comments/10wuxg6/london_irl_streamer_meets_the_king_of_england/", "name": "t1_j7s52x6", "author_flair_template_id": null, "subreddit_name_prefixed": "r/LivestreamFail", "author_flair_text": null, "treatment_tags": [], "created": 1675903159.0, "created_utc": 1675903159.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://clips.twitch.tv/EnticingEntertainingSowOhMyDog-v6M0MwF2jHnMdTni"}}, {"kind": "t1", "data": {"subreddit_id": "t5_38jf0", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "Starforge Logo = pen*s confirmed by the literal god himself. True and real.", "mod_reason_by": null, "banned_by": null, "ups": 3, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "LivestreamFail", "link_author": "Jarlice", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "j7dffqr", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "Hyper1on", "num_comments": 111, "can_mod_post": false, "send_replies": true, "parent_id": "t1_j7cm6y6", "score": 3, "author_fullname": "t2_7c7fe", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "My guess is:\n\n1. Convert many past Asmon clips to text using speech to text off the shelf programs.\n2. Train a text only AI model, to say things that Asmon would say using the transcripts from step 1.\n3. Collect the audio from these clips and feed them to an AI model to train it to speak in Asmon's voice, when given a piece of text to read.\n4. Use a lip sync model on video of Asmon's face + transcripts to (roughly) sync his lips to a piece of text being read out.\n5. When a new question comes in, prompt the text model with it, get a generated answer that is hopefully something Asmon would say. Then pipe that to the audio model for text to speech, get Asmon saying the answer. Then pipe that to the lip sync model and get a video sync'd up with the audio. Then send the result to the stream.\n\nThere are commercial services out there that will do steps 2, 3, and 4 for you I believe, if you have the data.", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;My guess is:&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Convert many past Asmon clips to text using speech to text off the shelf programs.&lt;/li&gt;\n&lt;li&gt;Train a text only AI model, to say things that Asmon would say using the transcripts from step 1.&lt;/li&gt;\n&lt;li&gt;Collect the audio from these clips and feed them to an AI model to train it to speak in Asmon&amp;#39;s voice, when given a piece of text to read.&lt;/li&gt;\n&lt;li&gt;Use a lip sync model on video of Asmon&amp;#39;s face + transcripts to (roughly) sync his lips to a piece of text being read out.&lt;/li&gt;\n&lt;li&gt;When a new question comes in, prompt the text model with it, get a generated answer that is hopefully something Asmon would say. Then pipe that to the audio model for text to speech, get Asmon saying the answer. Then pipe that to the lip sync model and get a video sync&amp;#39;d up with the audio. Then send the result to the stream.&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;There are commercial services out there that will do steps 2, 3, and 4 for you I believe, if you have the data.&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_10ulqaa", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/LivestreamFail/comments/10ulqaa/starforge_logo_pens_confirmed_by_the_literal_god/j7dffqr/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/LivestreamFail/comments/10ulqaa/starforge_logo_pens_confirmed_by_the_literal_god/", "name": "t1_j7dffqr", "author_flair_template_id": null, "subreddit_name_prefixed": "r/LivestreamFail", "author_flair_text": null, "treatment_tags": [], "created": 1675640823.0, "created_utc": 1675640823.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://clips.twitch.tv/FurryUgliestCroissantBrainSlug-Py_6wm_xGR2I29aq"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "[N] GitHub CEO on why open source developers should be exempt from the EU\u2019s AI Act", "mod_reason_by": null, "banned_by": null, "ups": 3, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "MachineLearning", "link_author": "EmbarrassedHelp", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "j7c7vga", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "Hyper1on", "num_comments": 41, "can_mod_post": false, "send_replies": true, "parent_id": "t1_j7bsnoo", "score": 3, "author_fullname": "t2_7c7fe", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "This isn't true - GDPR puts much more onerous restrictions on what consent must be gained before personal data is processed. Much of what cookies collect is considered personal data, and so immediately on GDPR's passing, many websites started to change their cookie acceptance boxes to these massive things which take up half the screen and have granular consent check boxes. Another factor which just makes browsing the web increasingly inconvenient for the average user.", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This isn&amp;#39;t true - GDPR puts much more onerous restrictions on what consent must be gained before personal data is processed. Much of what cookies collect is considered personal data, and so immediately on GDPR&amp;#39;s passing, many websites started to change their cookie acceptance boxes to these massive things which take up half the screen and have granular consent check boxes. Another factor which just makes browsing the web increasingly inconvenient for the average user.&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_10tjctk", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/MachineLearning/comments/10tjctk/n_github_ceo_on_why_open_source_developers_should/j7c7vga/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/MachineLearning/comments/10tjctk/n_github_ceo_on_why_open_source_developers_should/", "name": "t1_j7c7vga", "author_flair_template_id": null, "subreddit_name_prefixed": "r/MachineLearning", "author_flair_text": null, "treatment_tags": [], "created": 1675622658.0, "created_utc": 1675622658.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://techcrunch.com/2023/02/03/github-ceo-on-why-open-source-developers-should-be-exempt-from-the-eus-ai-act/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "[D]How Will Open Source Alternatives Compete With GPT3?", "mod_reason_by": null, "banned_by": null, "ups": 1, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "MachineLearning", "link_author": "noellarkin", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "j6xn5do", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "Hyper1on", "num_comments": 9, "can_mod_post": false, "send_replies": true, "parent_id": "t3_10rhprm", "score": 1, "author_fullname": "t2_7c7fe", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "&gt; AI21's Jurassic 178B seems to be comparable to GPT3 davinci 001.\n\nThis is actually a compliment to AI21, since davinci001 is fine-tuned from original 175B davinci on human feedback over generations:\n\nhttps://platform.openai.com/docs/model-index-for-researchers\n\nThe better comparison is with plain davinci, and you would expect 001 to be better and 003 to be significantly better (the latter is trained with RLHF).\n\nThere are currently no open source RLHF models to compete with davinci 003, but this will change in 2023.", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;AI21&amp;#39;s Jurassic 178B seems to be comparable to GPT3 davinci 001.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;This is actually a compliment to AI21, since davinci001 is fine-tuned from original 175B davinci on human feedback over generations:&lt;/p&gt;\n\n&lt;p&gt;&lt;a href=\"https://platform.openai.com/docs/model-index-for-researchers\"&gt;https://platform.openai.com/docs/model-index-for-researchers&lt;/a&gt;&lt;/p&gt;\n\n&lt;p&gt;The better comparison is with plain davinci, and you would expect 001 to be better and 003 to be significantly better (the latter is trained with RLHF).&lt;/p&gt;\n\n&lt;p&gt;There are currently no open source RLHF models to compete with davinci 003, but this will change in 2023.&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_10rhprm", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/MachineLearning/comments/10rhprm/dhow_will_open_source_alternatives_compete_with/j6xn5do/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/MachineLearning/comments/10rhprm/dhow_will_open_source_alternatives_compete_with/", "name": "t1_j6xn5do", "author_flair_template_id": null, "subreddit_name_prefixed": "r/MachineLearning", "author_flair_text": null, "treatment_tags": [], "created": 1675356854.0, "created_utc": 1675356854.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/MachineLearning/comments/10rhprm/dhow_will_open_source_alternatives_compete_with/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2r3gv", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "[R] Faithful Chain-of-Thought Reasoning", "mod_reason_by": null, "banned_by": null, "ups": 2, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "MachineLearning", "link_author": "starstruckmon", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "j6xm8m9", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "Hyper1on", "num_comments": 17, "can_mod_post": false, "send_replies": true, "parent_id": "t3_10qhgmv", "score": 2, "author_fullname": "t2_7c7fe", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "This is a fine approach, but it's not necessarily chain of thought if you move the actual problem solving outside of the LM. The entire point of Chain of Thought as originally conceived is that it's a better way of doing within-model problem solving.\nI would be interested to see the result if you were to finetune the LM on a dataset of reasoning from this approach, however.", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;This is a fine approach, but it&amp;#39;s not necessarily chain of thought if you move the actual problem solving outside of the LM. The entire point of Chain of Thought as originally conceived is that it&amp;#39;s a better way of doing within-model problem solving.\nI would be interested to see the result if you were to finetune the LM on a dataset of reasoning from this approach, however.&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_10qhgmv", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/MachineLearning/comments/10qhgmv/r_faithful_chainofthought_reasoning/j6xm8m9/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/MachineLearning/comments/10qhgmv/r_faithful_chainofthought_reasoning/", "name": "t1_j6xm8m9", "author_flair_template_id": null, "subreddit_name_prefixed": "r/MachineLearning", "author_flair_text": null, "treatment_tags": [], "created": 1675356519.0, "created_utc": 1675356519.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/MachineLearning/comments/10qhgmv/r_faithful_chainofthought_reasoning/"}}], "before": null}}