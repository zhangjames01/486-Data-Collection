{"kind": "Listing", "data": {"after": "t1_jblau6x", "dist": 25, "modhash": null, "geo_filter": "", "children": [{"kind": "t1", "data": {"subreddit_id": "t5_2ruui", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "Moving from BTRFS to ZFS - what things are different?", "mod_reason_by": null, "banned_by": null, "ups": 1, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "zfs", "link_author": "ranjop", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jcmxu12", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "mercenary_sysadmin", "num_comments": 25, "can_mod_post": false, "send_replies": true, "parent_id": "t1_jcmh7mb", "score": 1, "author_fullname": "t2_boter", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "&gt; On top of that, now you have to deal with ZFS padding. On a 4k disk (by ashift), the smallest file you can write will always consume a minimum of 8k. \n\nRight. The same 8K it would take on the same disks in any flavor of two copy mirror. Because that disk has 4K sectors, and you need to write two copies of the one sector.\n\n&gt; ZFS, if you plan to use it long term, can't be above 20% for extended periods\n\nTry filling a btrfs array to 100% sometime... \n\nZFS shouldn't generally be used at more than 80% capacity, yes .. but neither should any other filesystem.", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;On top of that, now you have to deal with ZFS padding. On a 4k disk (by ashift), the smallest file you can write will always consume a minimum of 8k. &lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Right. The same 8K it would take on the same disks in any flavor of two copy mirror. Because that disk has 4K sectors, and you need to write two copies of the one sector.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;ZFS, if you plan to use it long term, can&amp;#39;t be above 20% for extended periods&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Try filling a btrfs array to 100% sometime... &lt;/p&gt;\n\n&lt;p&gt;ZFS shouldn&amp;#39;t generally be used at more than 80% capacity, yes .. but neither should any other filesystem.&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11tm7vp", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/zfs/comments/11tm7vp/moving_from_btrfs_to_zfs_what_things_are_different/jcmxu12/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/zfs/comments/11tm7vp/moving_from_btrfs_to_zfs_what_things_are_different/", "name": "t1_jcmxu12", "author_flair_template_id": null, "subreddit_name_prefixed": "r/zfs", "author_flair_text": null, "treatment_tags": [], "created": 1679098431.0, "created_utc": 1679098431.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/zfs/comments/11tm7vp/moving_from_btrfs_to_zfs_what_things_are_different/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2ruui", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "Moving from BTRFS to ZFS - what things are different?", "mod_reason_by": null, "banned_by": null, "ups": 0, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "zfs", "link_author": "ranjop", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jcm9uti", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "mercenary_sysadmin", "num_comments": 25, "can_mod_post": false, "send_replies": true, "parent_id": "t1_jcm5yt7", "score": 0, "author_fullname": "t2_boter", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "&gt; No. They agree with me in principle.\n\nYou're going to have to do more to develop that case than just say \"they agree with me\" when their words do precisely the opposite, and strongly.\n\n&gt; systems that [...] are frequently backed up and the loss of the data in that period is not problematic. Like [...] hobbyist spaces.\n\nYou seem very unfamiliar with the reality of most hobbyist spaces.\n\n&gt; RAID5 with 3 disks is a bad idea for this case in particular, because migrating from Btrfs RAID1 with 3 disks will net him an extra 5% storage space if he is lucky\n\nThis is just plain incorrect.\n\nA 3-wide btrfs-raid1 of 10T disks gets you 15T after redundancy.\n\nA 3-wide RAIDz1 of 10T disks gets you 20T after parity.\n\nThat's a 33% bump in usable storage space, not 5%.", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;No. They agree with me in principle.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;You&amp;#39;re going to have to do more to develop that case than just say &amp;quot;they agree with me&amp;quot; when their words do precisely the opposite, and strongly.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;systems that [...] are frequently backed up and the loss of the data in that period is not problematic. Like [...] hobbyist spaces.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;You seem very unfamiliar with the reality of most hobbyist spaces.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;RAID5 with 3 disks is a bad idea for this case in particular, because migrating from Btrfs RAID1 with 3 disks will net him an extra 5% storage space if he is lucky&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;This is just plain incorrect.&lt;/p&gt;\n\n&lt;p&gt;A 3-wide btrfs-raid1 of 10T disks gets you 15T after redundancy.&lt;/p&gt;\n\n&lt;p&gt;A 3-wide RAIDz1 of 10T disks gets you 20T after parity.&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s a 33% bump in usable storage space, not 5%.&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11tm7vp", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/zfs/comments/11tm7vp/moving_from_btrfs_to_zfs_what_things_are_different/jcm9uti/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/zfs/comments/11tm7vp/moving_from_btrfs_to_zfs_what_things_are_different/", "name": "t1_jcm9uti", "author_flair_template_id": null, "subreddit_name_prefixed": "r/zfs", "author_flair_text": null, "treatment_tags": [], "created": 1679088113.0, "created_utc": 1679088113.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/zfs/comments/11tm7vp/moving_from_btrfs_to_zfs_what_things_are_different/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2ruui", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "Moving from BTRFS to ZFS - what things are different?", "mod_reason_by": null, "banned_by": null, "ups": 0, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "zfs", "link_author": "ranjop", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jcm0t99", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "mercenary_sysadmin", "num_comments": 25, "can_mod_post": false, "send_replies": true, "parent_id": "t1_jclmwhc", "score": 0, "author_fullname": "t2_boter", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "&gt; 4-5 disks is the sweet spot [for RAID5 arrays], generally speaking.\n\nPretty much the **entire** storage industry disagrees with you here. \n\nThe majority of the storage industry has deprecated single-parity striped arrays *entirely*, eg Dell: \"RAID5 is no longer recommended for any business critical information on any drive type,\" Arcserve: \"RAID5 is deprecated and should never be used in new arrays,\" ASK database management: \"RAID5 should be avoided at all costs,\" ZDNET: \"RAID5 stop[ped] working in 2009,\" and so forth.\n\nWhen I recommend RAIDz1 \"no wider than three disks\" I'm actually *already* making looser recommendations than the generally-accepted industry best practices, which I feel I can do largely because I'm specifically talking about ZFS (which is inherently more resilient to corruption than traditional RAID5 can be, and inherently far better performing as well).", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;4-5 disks is the sweet spot [for RAID5 arrays], generally speaking.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Pretty much the &lt;strong&gt;entire&lt;/strong&gt; storage industry disagrees with you here. &lt;/p&gt;\n\n&lt;p&gt;The majority of the storage industry has deprecated single-parity striped arrays &lt;em&gt;entirely&lt;/em&gt;, eg Dell: &amp;quot;RAID5 is no longer recommended for any business critical information on any drive type,&amp;quot; Arcserve: &amp;quot;RAID5 is deprecated and should never be used in new arrays,&amp;quot; ASK database management: &amp;quot;RAID5 should be avoided at all costs,&amp;quot; ZDNET: &amp;quot;RAID5 stop[ped] working in 2009,&amp;quot; and so forth.&lt;/p&gt;\n\n&lt;p&gt;When I recommend RAIDz1 &amp;quot;no wider than three disks&amp;quot; I&amp;#39;m actually &lt;em&gt;already&lt;/em&gt; making looser recommendations than the generally-accepted industry best practices, which I feel I can do largely because I&amp;#39;m specifically talking about ZFS (which is inherently more resilient to corruption than traditional RAID5 can be, and inherently far better performing as well).&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11tm7vp", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/zfs/comments/11tm7vp/moving_from_btrfs_to_zfs_what_things_are_different/jcm0t99/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/zfs/comments/11tm7vp/moving_from_btrfs_to_zfs_what_things_are_different/", "name": "t1_jcm0t99", "author_flair_template_id": null, "subreddit_name_prefixed": "r/zfs", "author_flair_text": null, "treatment_tags": [], "created": 1679084462.0, "created_utc": 1679084462.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/zfs/comments/11tm7vp/moving_from_btrfs_to_zfs_what_things_are_different/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2ruui", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "Moving from BTRFS to ZFS - what things are different?", "mod_reason_by": null, "banned_by": null, "ups": 1, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "zfs", "link_author": "ranjop", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jclj7sy", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "mercenary_sysadmin", "num_comments": 25, "can_mod_post": false, "send_replies": true, "parent_id": "t1_jcjsh7d", "score": 1, "author_fullname": "t2_boter", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "&gt; Also 3 disks is way too small for raid5. 4-6 it's kind of the norm.\n\nYou have this exactly backwards. Single parity striped arrays (zfs or not) should be **narrow**, for reasons of both reliability/fault-tolerance **and** performance.\n\nI do not advise raidz1 vdevs (or raid5 arrays!) more than three disks wide.", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Also 3 disks is way too small for raid5. 4-6 it&amp;#39;s kind of the norm.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;You have this exactly backwards. Single parity striped arrays (zfs or not) should be &lt;strong&gt;narrow&lt;/strong&gt;, for reasons of both reliability/fault-tolerance &lt;strong&gt;and&lt;/strong&gt; performance.&lt;/p&gt;\n\n&lt;p&gt;I do not advise raidz1 vdevs (or raid5 arrays!) more than three disks wide.&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11tm7vp", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/zfs/comments/11tm7vp/moving_from_btrfs_to_zfs_what_things_are_different/jclj7sy/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/zfs/comments/11tm7vp/moving_from_btrfs_to_zfs_what_things_are_different/", "name": "t1_jclj7sy", "author_flair_template_id": null, "subreddit_name_prefixed": "r/zfs", "author_flair_text": null, "treatment_tags": [], "created": 1679077564.0, "created_utc": 1679077564.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/zfs/comments/11tm7vp/moving_from_btrfs_to_zfs_what_things_are_different/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2ruui", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "Moving from BTRFS to ZFS - what things are different?", "mod_reason_by": null, "banned_by": null, "ups": 1, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "zfs", "link_author": "ranjop", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jckn4sd", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": false, "author": "mercenary_sysadmin", "num_comments": 25, "can_mod_post": false, "send_replies": true, "parent_id": "t3_11tm7vp", "score": 1, "author_fullname": "t2_boter", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "The biggest thing to wrap your head around is that zfs has its own namespace *separate* from the system mount hierarchy.\n\nWith btrfs, you can't really refer to an array separately from its member disks: \"mount /dev/sda\" will mount an entire btrfs array which includes /dev/sda, and so forth.\n\nUnder zfs, you have a proper internal namespace. So \"mount /dev/sda\" won't mount a ZFS pool which includes /dev/sda, because we understand that /dev/sda is not the same thing as the entire pool. Instead, we import or export the pool by its actual name.\n\nThis follows through to datasets and zvols, which are ZFS's version of what btrfs calls subvols. In btrfs, you must refer to a subvol by its mountpoint, because it doesn't really have a name of its own.\n\nBut in zfs, mountpoint is a property of the dataset, not its actual identifier. This means that you always know where to find a child dataset or snapshot: it's attached to its parent in the zfs namespace, so eg `zfs list -rt snapshot poolname/setname` will *always* find all the snapshots of the dataset `poolname/setname`.\n\nSimilarly, a full on child dataset of `poolname/setname` will always be found direct beneath it in the zfs namespace: eg `poolname/setname/child name/grandchildname` and so forth.\n\nBy default, you'll get very similar mountpoints: `poolname` will be mounted at `/poolname`, `poolname/setname` will be mounted at `/poolname/setname`, and so forth. But this can be adjusted, *without* needing to screw around with the zfs namespace, if you want it mounted somewhere different:\n\n`zfs set mountpoint=/lol poolname` will change `poolname`'s mountpoint from /poolname to /lol... And if poolname/setname doesn't have an explicitly set mountpoint, it'll inherit its parent's mountpoint as a prefix, so poolname/setname will now be mounted at /lol/setname.\n\nDon't like that either? No problem: `zfs set mountpoint=/lmao poolname/setname` and now poolname is mounted at /lol and poolname/setname is mounted at /lmao.\n\nBut notice the format of the zfs set commands doesn't change when the mountpoint does: poolname/setname might be *mounted* at /lmao now, but its position in the zfs namespace hasn't changed, so you still know where to find it, recursive zfs operations still operate on it as a child of poolname, and so forth.\n\nNow, look back over all this and note that when referring to zfs namespace, there's no leading slash: but when referring to the system mountpoints, there is a leading slash.\n\nSo poolname/setname is the zfs name of a dataset, but /poolname/setname is a system mountpoint (which is the default mountpoint of poolname/setname, if you haven't explicitly set a different mountpoint for either).\n\nThere are plenty of other differences, but this is probably the single biggest fundamental to get your head wrapped around, when coming from btrfs-land. :)", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;The biggest thing to wrap your head around is that zfs has its own namespace &lt;em&gt;separate&lt;/em&gt; from the system mount hierarchy.&lt;/p&gt;\n\n&lt;p&gt;With btrfs, you can&amp;#39;t really refer to an array separately from its member disks: &amp;quot;mount /dev/sda&amp;quot; will mount an entire btrfs array which includes /dev/sda, and so forth.&lt;/p&gt;\n\n&lt;p&gt;Under zfs, you have a proper internal namespace. So &amp;quot;mount /dev/sda&amp;quot; won&amp;#39;t mount a ZFS pool which includes /dev/sda, because we understand that /dev/sda is not the same thing as the entire pool. Instead, we import or export the pool by its actual name.&lt;/p&gt;\n\n&lt;p&gt;This follows through to datasets and zvols, which are ZFS&amp;#39;s version of what btrfs calls subvols. In btrfs, you must refer to a subvol by its mountpoint, because it doesn&amp;#39;t really have a name of its own.&lt;/p&gt;\n\n&lt;p&gt;But in zfs, mountpoint is a property of the dataset, not its actual identifier. This means that you always know where to find a child dataset or snapshot: it&amp;#39;s attached to its parent in the zfs namespace, so eg &lt;code&gt;zfs list -rt snapshot poolname/setname&lt;/code&gt; will &lt;em&gt;always&lt;/em&gt; find all the snapshots of the dataset &lt;code&gt;poolname/setname&lt;/code&gt;.&lt;/p&gt;\n\n&lt;p&gt;Similarly, a full on child dataset of &lt;code&gt;poolname/setname&lt;/code&gt; will always be found direct beneath it in the zfs namespace: eg &lt;code&gt;poolname/setname/child name/grandchildname&lt;/code&gt; and so forth.&lt;/p&gt;\n\n&lt;p&gt;By default, you&amp;#39;ll get very similar mountpoints: &lt;code&gt;poolname&lt;/code&gt; will be mounted at &lt;code&gt;/poolname&lt;/code&gt;, &lt;code&gt;poolname/setname&lt;/code&gt; will be mounted at &lt;code&gt;/poolname/setname&lt;/code&gt;, and so forth. But this can be adjusted, &lt;em&gt;without&lt;/em&gt; needing to screw around with the zfs namespace, if you want it mounted somewhere different:&lt;/p&gt;\n\n&lt;p&gt;&lt;code&gt;zfs set mountpoint=/lol poolname&lt;/code&gt; will change &lt;code&gt;poolname&lt;/code&gt;&amp;#39;s mountpoint from /poolname to /lol... And if poolname/setname doesn&amp;#39;t have an explicitly set mountpoint, it&amp;#39;ll inherit its parent&amp;#39;s mountpoint as a prefix, so poolname/setname will now be mounted at /lol/setname.&lt;/p&gt;\n\n&lt;p&gt;Don&amp;#39;t like that either? No problem: &lt;code&gt;zfs set mountpoint=/lmao poolname/setname&lt;/code&gt; and now poolname is mounted at /lol and poolname/setname is mounted at /lmao.&lt;/p&gt;\n\n&lt;p&gt;But notice the format of the zfs set commands doesn&amp;#39;t change when the mountpoint does: poolname/setname might be &lt;em&gt;mounted&lt;/em&gt; at /lmao now, but its position in the zfs namespace hasn&amp;#39;t changed, so you still know where to find it, recursive zfs operations still operate on it as a child of poolname, and so forth.&lt;/p&gt;\n\n&lt;p&gt;Now, look back over all this and note that when referring to zfs namespace, there&amp;#39;s no leading slash: but when referring to the system mountpoints, there is a leading slash.&lt;/p&gt;\n\n&lt;p&gt;So poolname/setname is the zfs name of a dataset, but /poolname/setname is a system mountpoint (which is the default mountpoint of poolname/setname, if you haven&amp;#39;t explicitly set a different mountpoint for either).&lt;/p&gt;\n\n&lt;p&gt;There are plenty of other differences, but this is probably the single biggest fundamental to get your head wrapped around, when coming from btrfs-land. :)&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": "moderator", "associated_award": null, "stickied": true, "author_premium": false, "can_gild": true, "link_id": "t3_11tm7vp", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": true, "permalink": "/r/zfs/comments/11tm7vp/moving_from_btrfs_to_zfs_what_things_are_different/jckn4sd/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/zfs/comments/11tm7vp/moving_from_btrfs_to_zfs_what_things_are_different/", "name": "t1_jckn4sd", "author_flair_template_id": null, "subreddit_name_prefixed": "r/zfs", "author_flair_text": null, "treatment_tags": [], "created": 1679065255.0, "created_utc": 1679065255.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/zfs/comments/11tm7vp/moving_from_btrfs_to_zfs_what_things_are_different/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2ruui", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "Running ZFS on OS X and seeing slow write performance issues causing whole system to hang", "mod_reason_by": null, "banned_by": null, "ups": 3, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "zfs", "link_author": "keen_cmdr", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jcfpcm4", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "mercenary_sysadmin", "num_comments": 14, "can_mod_post": false, "send_replies": true, "parent_id": "t1_jcde0a4", "score": 3, "author_fullname": "t2_boter", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "If you don't have any random access inside files, you want recordsize=1M.\n\nMirrors will SIGNIFICANTLY outperform a single RAIDz vdev, especially at this scale and on rust.", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;If you don&amp;#39;t have any random access inside files, you want recordsize=1M.&lt;/p&gt;\n\n&lt;p&gt;Mirrors will SIGNIFICANTLY outperform a single RAIDz vdev, especially at this scale and on rust.&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11s9m1b", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/zfs/comments/11s9m1b/running_zfs_on_os_x_and_seeing_slow_write/jcfpcm4/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/zfs/comments/11s9m1b/running_zfs_on_os_x_and_seeing_slow_write/", "name": "t1_jcfpcm4", "author_flair_template_id": null, "subreddit_name_prefixed": "r/zfs", "author_flair_text": null, "treatment_tags": [], "created": 1678977512.0, "created_utc": 1678977512.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/zfs/comments/11s9m1b/running_zfs_on_os_x_and_seeing_slow_write/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2ruui", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "Running ZFS on OS X and seeing slow write performance issues causing whole system to hang", "mod_reason_by": null, "banned_by": null, "ups": 9, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "zfs", "link_author": "keen_cmdr", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jcd5k2h", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": false, "author": "mercenary_sysadmin", "num_comments": 14, "can_mod_post": false, "send_replies": true, "parent_id": "t1_jccoo9a", "score": 9, "author_fullname": "t2_boter", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "&gt; HGST HDN726040ALE614\n\nThose are 512e drives, so you really want to make sure your ashift is set to 12, not 9. The 0 that you're showing is \"default\" which could mean pretty much anything, unfortunately.\n\nIf you check what the drive's firmware reports, ZFS generally will default to that. Which is fine as long as the drive tells the truth (512 bytes logical, 4K physical) but if it lies--like Samsung consumer SSDs do--you'll end up with a horrible misconfiguration.\n\nIf you don't have any DB or VM stuff, you might want to consider a larger recordsize.\n\nFinally, 4-drive z1 isn't really ideal. Looks like you're not using compression, which means every stripe you write requires padding (because you can't split 128KiB evenly by three).\n\nAll of these issues will impact rust drives worse than SSDs, because the rust drives have worse failure modes than SSDs do in terms of how many orders of magnitude slower they are with challenging workloads vs simple ones.", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;HGST HDN726040ALE614&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;Those are 512e drives, so you really want to make sure your ashift is set to 12, not 9. The 0 that you&amp;#39;re showing is &amp;quot;default&amp;quot; which could mean pretty much anything, unfortunately.&lt;/p&gt;\n\n&lt;p&gt;If you check what the drive&amp;#39;s firmware reports, ZFS generally will default to that. Which is fine as long as the drive tells the truth (512 bytes logical, 4K physical) but if it lies--like Samsung consumer SSDs do--you&amp;#39;ll end up with a horrible misconfiguration.&lt;/p&gt;\n\n&lt;p&gt;If you don&amp;#39;t have any DB or VM stuff, you might want to consider a larger recordsize.&lt;/p&gt;\n\n&lt;p&gt;Finally, 4-drive z1 isn&amp;#39;t really ideal. Looks like you&amp;#39;re not using compression, which means every stripe you write requires padding (because you can&amp;#39;t split 128KiB evenly by three).&lt;/p&gt;\n\n&lt;p&gt;All of these issues will impact rust drives worse than SSDs, because the rust drives have worse failure modes than SSDs do in terms of how many orders of magnitude slower they are with challenging workloads vs simple ones.&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11s9m1b", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/zfs/comments/11s9m1b/running_zfs_on_os_x_and_seeing_slow_write/jcd5k2h/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/zfs/comments/11s9m1b/running_zfs_on_os_x_and_seeing_slow_write/", "name": "t1_jcd5k2h", "author_flair_template_id": null, "subreddit_name_prefixed": "r/zfs", "author_flair_text": null, "treatment_tags": [], "created": 1678924037.0, "created_utc": 1678924037.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/zfs/comments/11s9m1b/running_zfs_on_os_x_and_seeing_slow_write/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2ruui", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "Running ZFS on OS X and seeing slow write performance issues causing whole system to hang", "mod_reason_by": null, "banned_by": null, "ups": 1, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "zfs", "link_author": "keen_cmdr", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jccnt2o", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "mercenary_sysadmin", "num_comments": 14, "can_mod_post": false, "send_replies": true, "parent_id": "t3_11s9m1b", "score": 1, "author_fullname": "t2_boter", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "What model are the drives, and what's your pool ashift?\n\nAre you only doing simple file service from this pool, or do you have databases, VM images, stuff that requires random access *inside* files?", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;What model are the drives, and what&amp;#39;s your pool ashift?&lt;/p&gt;\n\n&lt;p&gt;Are you only doing simple file service from this pool, or do you have databases, VM images, stuff that requires random access &lt;em&gt;inside&lt;/em&gt; files?&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11s9m1b", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/zfs/comments/11s9m1b/running_zfs_on_os_x_and_seeing_slow_write/jccnt2o/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/zfs/comments/11s9m1b/running_zfs_on_os_x_and_seeing_slow_write/", "name": "t1_jccnt2o", "author_flair_template_id": null, "subreddit_name_prefixed": "r/zfs", "author_flair_text": null, "treatment_tags": [], "created": 1678916729.0, "created_utc": 1678916729.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/zfs/comments/11s9m1b/running_zfs_on_os_x_and_seeing_slow_write/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2ruui", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "Make ZFS hot spares less overzealous", "mod_reason_by": null, "banned_by": null, "ups": 4, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "zfs", "link_author": "mishac", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jc87r2k", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": false, "author": "mercenary_sysadmin", "num_comments": 23, "can_mod_post": false, "send_replies": true, "parent_id": "t1_jc867s4", "score": 4, "author_fullname": "t2_boter", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "Task blocked for 120 seconds is enough to convince zfs to throw a disk, if there is sufficient parity (or `SPARE`s) to allow it to operate without the disk it thinks isn't responding.\n\n120 seconds is a **long** time!", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Task blocked for 120 seconds is enough to convince zfs to throw a disk, if there is sufficient parity (or &lt;code&gt;SPARE&lt;/code&gt;s) to allow it to operate without the disk it thinks isn&amp;#39;t responding.&lt;/p&gt;\n\n&lt;p&gt;120 seconds is a &lt;strong&gt;long&lt;/strong&gt; time!&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11qbln9", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/zfs/comments/11qbln9/make_zfs_hot_spares_less_overzealous/jc87r2k/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/zfs/comments/11qbln9/make_zfs_hot_spares_less_overzealous/", "name": "t1_jc87r2k", "author_flair_template_id": null, "subreddit_name_prefixed": "r/zfs", "author_flair_text": null, "treatment_tags": [], "created": 1678839491.0, "created_utc": 1678839491.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/zfs/comments/11qbln9/make_zfs_hot_spares_less_overzealous/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2ruui", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "Make ZFS hot spares less overzealous", "mod_reason_by": null, "banned_by": null, "ups": 3, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "zfs", "link_author": "mishac", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jc8610h", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "mercenary_sysadmin", "num_comments": 23, "can_mod_post": false, "send_replies": true, "parent_id": "t3_11qbln9", "score": 3, "author_fullname": "t2_boter", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "&gt; I have replaced the SFF-8088 cable, and swapped the HBA card, so those don't seem to be at issue here.\n&gt; \n&gt; I have another pool attached via the same LSI card, in a DIY JBOD with an HP SAS expander, and that pool does not exhibit the same issues\n\nYou eliminated cabling, and another pool on the same system and same card isn't exhibiting the problem.\n\nWhat's different between the two pools?\n\n1. Backplane\n2. (Most likely, though you didn't really specify explicitly) power supply\n\nSo your culprit is very likely to be one of the two.\n\nYou mention in a comment getting a lot of \"task blocked for 120 seconds\". That's **not** normal; generally it indicates either a hardware problem or a bug. And since you've got other pools on the same system unaffected, a bug seems unlikely in one pool but not the other, so... Backplane or power supply.", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;I have replaced the SFF-8088 cable, and swapped the HBA card, so those don&amp;#39;t seem to be at issue here.&lt;/p&gt;\n\n&lt;p&gt;I have another pool attached via the same LSI card, in a DIY JBOD with an HP SAS expander, and that pool does not exhibit the same issues&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;You eliminated cabling, and another pool on the same system and same card isn&amp;#39;t exhibiting the problem.&lt;/p&gt;\n\n&lt;p&gt;What&amp;#39;s different between the two pools?&lt;/p&gt;\n\n&lt;ol&gt;\n&lt;li&gt;Backplane&lt;/li&gt;\n&lt;li&gt;(Most likely, though you didn&amp;#39;t really specify explicitly) power supply&lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;So your culprit is very likely to be one of the two.&lt;/p&gt;\n\n&lt;p&gt;You mention in a comment getting a lot of &amp;quot;task blocked for 120 seconds&amp;quot;. That&amp;#39;s &lt;strong&gt;not&lt;/strong&gt; normal; generally it indicates either a hardware problem or a bug. And since you&amp;#39;ve got other pools on the same system unaffected, a bug seems unlikely in one pool but not the other, so... Backplane or power supply.&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11qbln9", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/zfs/comments/11qbln9/make_zfs_hot_spares_less_overzealous/jc8610h/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/zfs/comments/11qbln9/make_zfs_hot_spares_less_overzealous/", "name": "t1_jc8610h", "author_flair_template_id": null, "subreddit_name_prefixed": "r/zfs", "author_flair_text": null, "treatment_tags": [], "created": 1678837346.0, "created_utc": 1678837346.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/zfs/comments/11qbln9/make_zfs_hot_spares_less_overzealous/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2s4tv", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "Mikhaila Peterson (JBP's daughter) with Andrew Tate", "mod_reason_by": null, "banned_by": null, "ups": 1, "num_reports": null, "author_flair_type": "richtext", "total_awards_received": 0, "subreddit": "JoeRogan", "link_author": "WolfilaTotilaAttila", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jc73htc", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "mercenary_sysadmin", "num_comments": 675, "can_mod_post": false, "send_replies": true, "parent_id": "t1_j7m88x0", "score": 1, "author_fullname": "t2_boter", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "Constipation is a common side effect of steroid use. That was probably either his coded way of saying \"I'm natural bro\"... Or him assuming you know he's juicing and might be worried about his condition, idk \ud83d\ude43", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [{"e": "text", "t": "Monkey in Space"}], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Constipation is a common side effect of steroid use. That was probably either his coded way of saying &amp;quot;I&amp;#39;m natural bro&amp;quot;... Or him assuming you know he&amp;#39;s juicing and might be worried about his condition, idk \ud83d\ude43&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_103ciqn", "unrepliable_reason": null, "author_flair_text_color": "dark", "score_hidden": false, "permalink": "/r/JoeRogan/comments/103ciqn/mikhaila_peterson_jbps_daughter_with_andrew_tate/jc73htc/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/JoeRogan/comments/103ciqn/mikhaila_peterson_jbps_daughter_with_andrew_tate/", "name": "t1_jc73htc", "author_flair_template_id": "89b811e6-a92f-11eb-92b4-0e0580462801", "subreddit_name_prefixed": "r/JoeRogan", "author_flair_text": "Monkey in Space", "treatment_tags": [], "created": 1678806223.0, "created_utc": 1678806223.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": "#94e044", "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://i.redd.it/cuywylbnv2aa1.jpg"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2ruui", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "Help understanding weird fio throughput on zfs versus ext4 throughput, iops and latency comparison.", "mod_reason_by": null, "banned_by": null, "ups": 1, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "zfs", "link_author": "arjunkc", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jbz3zh8", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "mercenary_sysadmin", "num_comments": 26, "can_mod_post": false, "send_replies": true, "parent_id": "t1_jbyx4gt", "score": 1, "author_fullname": "t2_boter", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "&gt; bs=(R) 4096B-4096B, (W) 4096B-4096B\n\nThese are bs=4K tests, not bs=64K tests. Among other problems (like the test in the middle that ENOSPC'd out).", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;bs=(R) 4096B-4096B, (W) 4096B-4096B&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;These are bs=4K tests, not bs=64K tests. Among other problems (like the test in the middle that ENOSPC&amp;#39;d out).&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11nbeiw", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/jbz3zh8/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/", "name": "t1_jbz3zh8", "author_flair_template_id": null, "subreddit_name_prefixed": "r/zfs", "author_flair_text": null, "treatment_tags": [], "created": 1678655038.0, "created_utc": 1678655038.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2ruui", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "Help understanding weird fio throughput on zfs versus ext4 throughput, iops and latency comparison.", "mod_reason_by": null, "banned_by": null, "ups": 1, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "zfs", "link_author": "arjunkc", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jbtdqym", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "mercenary_sysadmin", "num_comments": 26, "can_mod_post": false, "send_replies": true, "parent_id": "t1_jbsmli6", "score": 1, "author_fullname": "t2_boter", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "They'll be large enough to drive the average latency up significantly enough that just comparing average latencies between ashift values will generally be sufficient.\n\nYou can also look at maximum latency values on each test, of course. But you can have just one really bad outcome in a thirty second test without really affecting overall performance, so it's typically better to look at the averages... And if you really want to get into the weeds, look at eg the 90th percentile latency instead (there's a poor man's histogram built into the default output, if you look through it).", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;They&amp;#39;ll be large enough to drive the average latency up significantly enough that just comparing average latencies between ashift values will generally be sufficient.&lt;/p&gt;\n\n&lt;p&gt;You can also look at maximum latency values on each test, of course. But you can have just one really bad outcome in a thirty second test without really affecting overall performance, so it&amp;#39;s typically better to look at the averages... And if you really want to get into the weeds, look at eg the 90th percentile latency instead (there&amp;#39;s a poor man&amp;#39;s histogram built into the default output, if you look through it).&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11nbeiw", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/jbtdqym/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/", "name": "t1_jbtdqym", "author_flair_template_id": null, "subreddit_name_prefixed": "r/zfs", "author_flair_text": null, "treatment_tags": [], "created": 1678549812.0, "created_utc": 1678549812.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2ruui", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "Help understanding weird fio throughput on zfs versus ext4 throughput, iops and latency comparison.", "mod_reason_by": null, "banned_by": null, "ups": 1, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "zfs", "link_author": "arjunkc", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jbrgmj8", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "mercenary_sysadmin", "num_comments": 26, "can_mod_post": false, "send_replies": true, "parent_id": "t1_jbrcggt", "score": 1, "author_fullname": "t2_boter", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "iodepth=1 is useful if you want to learn the latency inherit in the metal, as configured.\n\niodepth=8 (or whatever) is useful if you want to learn the latency inherit in your io scheduler when it gets busy.\n\nWhich one more directly models your actual workload depends very strongly *on* your actual workload.\n\n&gt; Also, how should I capture the fio output\n\nYou can do all sorts of fancy things, but unless it's a very extended benchmarking project I generally just cherry pick a few stats I'm particularly interested in from each run, then plug them into a spreadsheet (Google sheets, usually).\n\nI put a copy of the exact fio command I'm running along with some notes on what I'm trying to learn, operating parameters, etc in the top of the sheet, then plug the data into a table with columns like \"read bw | read lat | write bw | write lat\" and rows like ashift=9, ashift=12, ashift=13, and so forth.\n\nIf I want to make something pretty, I can easily build a chart from the data in the table. But usually, the raw numbers in a simple table is enough visualization for me, if it's testing for my own understanding only and not for publication.", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;iodepth=1 is useful if you want to learn the latency inherit in the metal, as configured.&lt;/p&gt;\n\n&lt;p&gt;iodepth=8 (or whatever) is useful if you want to learn the latency inherit in your io scheduler when it gets busy.&lt;/p&gt;\n\n&lt;p&gt;Which one more directly models your actual workload depends very strongly &lt;em&gt;on&lt;/em&gt; your actual workload.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Also, how should I capture the fio output&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;You can do all sorts of fancy things, but unless it&amp;#39;s a very extended benchmarking project I generally just cherry pick a few stats I&amp;#39;m particularly interested in from each run, then plug them into a spreadsheet (Google sheets, usually).&lt;/p&gt;\n\n&lt;p&gt;I put a copy of the exact fio command I&amp;#39;m running along with some notes on what I&amp;#39;m trying to learn, operating parameters, etc in the top of the sheet, then plug the data into a table with columns like &amp;quot;read bw | read lat | write bw | write lat&amp;quot; and rows like ashift=9, ashift=12, ashift=13, and so forth.&lt;/p&gt;\n\n&lt;p&gt;If I want to make something pretty, I can easily build a chart from the data in the table. But usually, the raw numbers in a simple table is enough visualization for me, if it&amp;#39;s testing for my own understanding only and not for publication.&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11nbeiw", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/jbrgmj8/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/", "name": "t1_jbrgmj8", "author_flair_template_id": null, "subreddit_name_prefixed": "r/zfs", "author_flair_text": null, "treatment_tags": [], "created": 1678504816.0, "created_utc": 1678504816.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2ruui", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "Help understanding weird fio throughput on zfs versus ext4 throughput, iops and latency comparison.", "mod_reason_by": null, "banned_by": null, "ups": 1, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "zfs", "link_author": "arjunkc", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jbr1nmu", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "mercenary_sysadmin", "num_comments": 26, "can_mod_post": false, "send_replies": true, "parent_id": "t1_jbptgqe", "score": 1, "author_fullname": "t2_boter", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "As far as contaminating read queries with on-cache results, you can eliminate that simply with `zfs set primarycache=metadata` on the dataset you're testing inside.\n\nThat's not *entirely* accurate for real-world modeling, since in the real world you will get the benefit of *some* cache hits. But it gets real, real weird trying to accurately model the number of read cache hits you'd expect to really get on any given workload, so it tends to be simpler to just bench the metal itself and figure any cache hit in prod is a good cache hit. :)", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;As far as contaminating read queries with on-cache results, you can eliminate that simply with &lt;code&gt;zfs set primarycache=metadata&lt;/code&gt; on the dataset you&amp;#39;re testing inside.&lt;/p&gt;\n\n&lt;p&gt;That&amp;#39;s not &lt;em&gt;entirely&lt;/em&gt; accurate for real-world modeling, since in the real world you will get the benefit of &lt;em&gt;some&lt;/em&gt; cache hits. But it gets real, real weird trying to accurately model the number of read cache hits you&amp;#39;d expect to really get on any given workload, so it tends to be simpler to just bench the metal itself and figure any cache hit in prod is a good cache hit. :)&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11nbeiw", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/jbr1nmu/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/", "name": "t1_jbr1nmu", "author_flair_template_id": null, "subreddit_name_prefixed": "r/zfs", "author_flair_text": null, "treatment_tags": [], "created": 1678497417.0, "created_utc": 1678497417.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2ruui", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "Help understanding weird fio throughput on zfs versus ext4 throughput, iops and latency comparison.", "mod_reason_by": null, "banned_by": null, "ups": 1, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "zfs", "link_author": "arjunkc", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jbp4sgd", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "mercenary_sysadmin", "num_comments": 26, "can_mod_post": false, "send_replies": true, "parent_id": "t1_jbp1kxx", "score": 1, "author_fullname": "t2_boter", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "&gt; retest with with ashift=13\n\nI'd strongly recommend trying both 13 and 14. 14 wasn't supported when I last thoroughly tested these drives, but to my understanding they've likely got 16K page size, which should typically mean best performance at ashift=14 to match.\n\nSSDs are *much* more complex internally than rust drives are, though, so you should test both ways instead of blindly assuming; I've encountered some strange cases in the field.\n\nI recommend a 64K recordsize and fio blocksize, sequential write, with fsync=1 (not just end_fsync=1), to test for this specific issue (ideal ashift for a given drive). And ideally you want to pay more attention to latency than to throughput--generally speaking, they correlate inversely, so looking for higher throughput usually gets the job done, but latency is the actual key metric, and throughput's inverse relationship with it is complex enough that it's a better idea to make sure you're actually looking at latency instead of ignoring it.\n\nWhat you're doing with this test is issuing relatively small, sequential sync writes and looking for low, consistent latency. While ashift is lower than the actual page size, you get weird, bursty throughput and high, spiky latency due to the drive needing to issue read/modify/write cycles sometimes; but once ashift matches page/sector size, you have no more RMW and the latency spikes drop significantly.\n\nThis gets complicated by the fact that the SSD's firmware is aware of the issue, and tries to compensate by buffering writes in its RAM for long enough to be able to write complete pages, rather than having to write a partial page out, then read it back in later to put more data at the end of it then write it back out.\n\nBut the firmware isn't *so* clever that you can't still spot the latency spikes (or, in production, the bad performance in general) from mismatched ashift and page/sector size.", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;retest with with ashift=13&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;I&amp;#39;d strongly recommend trying both 13 and 14. 14 wasn&amp;#39;t supported when I last thoroughly tested these drives, but to my understanding they&amp;#39;ve likely got 16K page size, which should typically mean best performance at ashift=14 to match.&lt;/p&gt;\n\n&lt;p&gt;SSDs are &lt;em&gt;much&lt;/em&gt; more complex internally than rust drives are, though, so you should test both ways instead of blindly assuming; I&amp;#39;ve encountered some strange cases in the field.&lt;/p&gt;\n\n&lt;p&gt;I recommend a 64K recordsize and fio blocksize, sequential write, with fsync=1 (not just end_fsync=1), to test for this specific issue (ideal ashift for a given drive). And ideally you want to pay more attention to latency than to throughput--generally speaking, they correlate inversely, so looking for higher throughput usually gets the job done, but latency is the actual key metric, and throughput&amp;#39;s inverse relationship with it is complex enough that it&amp;#39;s a better idea to make sure you&amp;#39;re actually looking at latency instead of ignoring it.&lt;/p&gt;\n\n&lt;p&gt;What you&amp;#39;re doing with this test is issuing relatively small, sequential sync writes and looking for low, consistent latency. While ashift is lower than the actual page size, you get weird, bursty throughput and high, spiky latency due to the drive needing to issue read/modify/write cycles sometimes; but once ashift matches page/sector size, you have no more RMW and the latency spikes drop significantly.&lt;/p&gt;\n\n&lt;p&gt;This gets complicated by the fact that the SSD&amp;#39;s firmware is aware of the issue, and tries to compensate by buffering writes in its RAM for long enough to be able to write complete pages, rather than having to write a partial page out, then read it back in later to put more data at the end of it then write it back out.&lt;/p&gt;\n\n&lt;p&gt;But the firmware isn&amp;#39;t &lt;em&gt;so&lt;/em&gt; clever that you can&amp;#39;t still spot the latency spikes (or, in production, the bad performance in general) from mismatched ashift and page/sector size.&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11nbeiw", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/jbp4sgd/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/", "name": "t1_jbp4sgd", "author_flair_template_id": null, "subreddit_name_prefixed": "r/zfs", "author_flair_text": null, "treatment_tags": [], "created": 1678469065.0, "created_utc": 1678469065.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2ruui", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "Help understanding weird fio throughput on zfs versus ext4 throughput, iops and latency comparison.", "mod_reason_by": null, "banned_by": null, "ups": 2, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "zfs", "link_author": "arjunkc", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jbp2szc", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "mercenary_sysadmin", "num_comments": 26, "can_mod_post": false, "send_replies": true, "parent_id": "t1_jbp1kxx", "score": 2, "author_fullname": "t2_boter", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "Yes, I got frustrated and let that get the better of me. Sorry. \ud83d\ude13", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yes, I got frustrated and let that get the better of me. Sorry. \ud83d\ude13&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11nbeiw", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/jbp2szc/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/", "name": "t1_jbp2szc", "author_flair_template_id": null, "subreddit_name_prefixed": "r/zfs", "author_flair_text": null, "treatment_tags": [], "created": 1678468299.0, "created_utc": 1678468299.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2ruui", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "Help understanding weird fio throughput on zfs versus ext4 throughput, iops and latency comparison.", "mod_reason_by": null, "banned_by": null, "ups": 1, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "zfs", "link_author": "arjunkc", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jborl52", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "mercenary_sysadmin", "num_comments": 26, "can_mod_post": false, "send_replies": true, "parent_id": "t1_jbop0fj", "score": 1, "author_fullname": "t2_boter", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "Are you actually reading what I'm saying to you?\n\n**edit:** this is getting a lot snarkier than I intended, for which I sincerely apologize. Regardless, the point here is that ashift=12 is **not** the optimal setting for Samsung EVO (or Pro!) consumer SSDs. So you've got a misconfiguration on the ZFS side that you literally can't have on the ext4 side.\n\nThere are other variables in play when you compare and contrast performance on the two systems, but until you account for this one there's no point in going looking for the rest. There are similar issues with your choice of fio parameters; you're contaminating your results with extremely unrealistic amounts of read cache hits, along with falling to specify end_fsync=1 in your tests which include write benchmarking (thereby allowing the tests to \"complete\" while the data to be written is still in RAM, rather than waiting for it to actually be committed to disk).", "edited": 1678468234.0, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Are you actually reading what I&amp;#39;m saying to you?&lt;/p&gt;\n\n&lt;p&gt;&lt;strong&gt;edit:&lt;/strong&gt; this is getting a lot snarkier than I intended, for which I sincerely apologize. Regardless, the point here is that ashift=12 is &lt;strong&gt;not&lt;/strong&gt; the optimal setting for Samsung EVO (or Pro!) consumer SSDs. So you&amp;#39;ve got a misconfiguration on the ZFS side that you literally can&amp;#39;t have on the ext4 side.&lt;/p&gt;\n\n&lt;p&gt;There are other variables in play when you compare and contrast performance on the two systems, but until you account for this one there&amp;#39;s no point in going looking for the rest. There are similar issues with your choice of fio parameters; you&amp;#39;re contaminating your results with extremely unrealistic amounts of read cache hits, along with falling to specify end_fsync=1 in your tests which include write benchmarking (thereby allowing the tests to &amp;quot;complete&amp;quot; while the data to be written is still in RAM, rather than waiting for it to actually be committed to disk).&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11nbeiw", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/jborl52/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/", "name": "t1_jborl52", "author_flair_template_id": null, "subreddit_name_prefixed": "r/zfs", "author_flair_text": null, "treatment_tags": [], "created": 1678463981.0, "created_utc": 1678463981.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2ruui", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "Help understanding weird fio throughput on zfs versus ext4 throughput, iops and latency comparison.", "mod_reason_by": null, "banned_by": null, "ups": 1, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "zfs", "link_author": "arjunkc", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jbooap2", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "mercenary_sysadmin", "num_comments": 26, "can_mod_post": false, "send_replies": true, "parent_id": "t1_jbonsx9", "score": 1, "author_fullname": "t2_boter", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "Ext4 pretty obviously didn't have the wrong ashift value, given that ext4 doesn't *use* ashift values in the first place.", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Ext4 pretty obviously didn&amp;#39;t have the wrong ashift value, given that ext4 doesn&amp;#39;t &lt;em&gt;use&lt;/em&gt; ashift values in the first place.&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11nbeiw", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/jbooap2/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/", "name": "t1_jbooap2", "author_flair_template_id": null, "subreddit_name_prefixed": "r/zfs", "author_flair_text": null, "treatment_tags": [], "created": 1678462697.0, "created_utc": 1678462697.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2ruui", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "Help understanding weird fio throughput on zfs versus ext4 throughput, iops and latency comparison.", "mod_reason_by": null, "banned_by": null, "ups": 3, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "zfs", "link_author": "arjunkc", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jbonzku", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "mercenary_sysadmin", "num_comments": 26, "can_mod_post": false, "send_replies": true, "parent_id": "t1_jbon4gx", "score": 3, "author_fullname": "t2_boter", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "ashift=12 is still low for those drives. When I started using Samsung consumer SSDs, 13 was the highest value of ashift possible without screwing around with zfs internals, and that's what I used because it performed significantly better than 12.\n\nBut to the best of my knowledge, the real page size on those drives is 16KiB, not 8KiB. I haven't retested since ashift=14 became directly available in ZFS, so I'm not sure whether to tell you to use 13 or 14 for best performance... But you're definitely not optimal at 12.", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;ashift=12 is still low for those drives. When I started using Samsung consumer SSDs, 13 was the highest value of ashift possible without screwing around with zfs internals, and that&amp;#39;s what I used because it performed significantly better than 12.&lt;/p&gt;\n\n&lt;p&gt;But to the best of my knowledge, the real page size on those drives is 16KiB, not 8KiB. I haven&amp;#39;t retested since ashift=14 became directly available in ZFS, so I&amp;#39;m not sure whether to tell you to use 13 or 14 for best performance... But you&amp;#39;re definitely not optimal at 12.&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11nbeiw", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/jbonzku/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/", "name": "t1_jbonzku", "author_flair_template_id": null, "subreddit_name_prefixed": "r/zfs", "author_flair_text": null, "treatment_tags": [], "created": 1678462575.0, "created_utc": 1678462575.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2ruui", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "Help understanding weird fio throughput on zfs versus ext4 throughput, iops and latency comparison.", "mod_reason_by": null, "banned_by": null, "ups": 4, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "zfs", "link_author": "arjunkc", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jboi33p", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": false, "author": "mercenary_sysadmin", "num_comments": 26, "can_mod_post": false, "send_replies": true, "parent_id": "t1_jbo3scg", "score": 4, "author_fullname": "t2_boter", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "Sort of. If you set both fio and recordsize to 4K, you're accurately modeling metadata operation performance and 4K individual file operation performance on your pool.\n\nBut it's important to realize that you don't need to set RS=4K on real datasets to get that level of performance. You get that automatically, because small files and metadata get minimally sized blocks even when recordsize is (much) larger.\n\nSo for real world workloads, you don't want tiny recordsize unless you're working with a storage workload that does tiny blocksize random I/O *inside* much larger files. Which is how both database engines and fio work, which is why you need to set RS=4K for fio even though you don't need it for real ops on metadata and on 4K files!", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Sort of. If you set both fio and recordsize to 4K, you&amp;#39;re accurately modeling metadata operation performance and 4K individual file operation performance on your pool.&lt;/p&gt;\n\n&lt;p&gt;But it&amp;#39;s important to realize that you don&amp;#39;t need to set RS=4K on real datasets to get that level of performance. You get that automatically, because small files and metadata get minimally sized blocks even when recordsize is (much) larger.&lt;/p&gt;\n\n&lt;p&gt;So for real world workloads, you don&amp;#39;t want tiny recordsize unless you&amp;#39;re working with a storage workload that does tiny blocksize random I/O &lt;em&gt;inside&lt;/em&gt; much larger files. Which is how both database engines and fio work, which is why you need to set RS=4K for fio even though you don&amp;#39;t need it for real ops on metadata and on 4K files!&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11nbeiw", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/jboi33p/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/", "name": "t1_jboi33p", "author_flair_template_id": null, "subreddit_name_prefixed": "r/zfs", "author_flair_text": null, "treatment_tags": [], "created": 1678460217.0, "created_utc": 1678460217.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2ruui", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "Help understanding weird fio throughput on zfs versus ext4 throughput, iops and latency comparison.", "mod_reason_by": null, "banned_by": null, "ups": 2, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "zfs", "link_author": "arjunkc", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jboh7me", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "mercenary_sysadmin", "num_comments": 26, "can_mod_post": false, "send_replies": true, "parent_id": "t3_11nbeiw", "score": 2, "author_fullname": "t2_boter", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "&gt; Sequential read \tzfsonluks and plainzfs report nearly 4000 MB/s on an 850 Evo sata ssd.\n\nYou're benchmarking your RAM. The EVO isn't capable of anything even close to that level of throughput.\n\n&gt; Random read/write \tThe random read-write tests are all a bit wonky\n\nThat's because you're using an EVO SSD, which relies on easily-exhaustible onboard cache to provide adequate performance. Once you burn through that cache and start issuing writes directly to its main storage, performance plummets off a cliff--not only for writes, but for any reads that have to hit the metal also, because you have to service both reads and writes out of the same stressed-out storage (essentially, it's half duplex, not full duplex).\n\nYou may also have the wrong ashift setting. You didn't say anything about ashift, but Samsung consumer SSDs lie to the operating system and pretend to be 512B sector drives when they're really 512e, and in fact perform like hot garbage if asked to operate in 512e mode. But since they lie about themselves to the OS, zfs will give them ashift=9 if not manually overridden at vdev creation time (eg zpool create -oashift=13 or zpool add -oashift=13).\n\nRemember, ashift is per-vdev, **not** per-zpool: so you need to specify this manually when working with Samsung consumer SSDs whenever you add a new vdev, as well as whenever you create an entirely new pool.", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Sequential read   zfsonluks and plainzfs report nearly 4000 MB/s on an 850 Evo sata ssd.&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;You&amp;#39;re benchmarking your RAM. The EVO isn&amp;#39;t capable of anything even close to that level of throughput.&lt;/p&gt;\n\n&lt;blockquote&gt;\n&lt;p&gt;Random read/write     The random read-write tests are all a bit wonky&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;That&amp;#39;s because you&amp;#39;re using an EVO SSD, which relies on easily-exhaustible onboard cache to provide adequate performance. Once you burn through that cache and start issuing writes directly to its main storage, performance plummets off a cliff--not only for writes, but for any reads that have to hit the metal also, because you have to service both reads and writes out of the same stressed-out storage (essentially, it&amp;#39;s half duplex, not full duplex).&lt;/p&gt;\n\n&lt;p&gt;You may also have the wrong ashift setting. You didn&amp;#39;t say anything about ashift, but Samsung consumer SSDs lie to the operating system and pretend to be 512B sector drives when they&amp;#39;re really 512e, and in fact perform like hot garbage if asked to operate in 512e mode. But since they lie about themselves to the OS, zfs will give them ashift=9 if not manually overridden at vdev creation time (eg zpool create -oashift=13 or zpool add -oashift=13).&lt;/p&gt;\n\n&lt;p&gt;Remember, ashift is per-vdev, &lt;strong&gt;not&lt;/strong&gt; per-zpool: so you need to specify this manually when working with Samsung consumer SSDs whenever you add a new vdev, as well as whenever you create an entirely new pool.&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11nbeiw", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/jboh7me/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/", "name": "t1_jboh7me", "author_flair_template_id": null, "subreddit_name_prefixed": "r/zfs", "author_flair_text": null, "treatment_tags": [], "created": 1678459859.0, "created_utc": 1678459859.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/zfs/comments/11nbeiw/help_understanding_weird_fio_throughput_on_zfs/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2ruui", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "How do you install the bootloader on a zfsbootmenu replacement disk?", "mod_reason_by": null, "banned_by": null, "ups": 1, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "zfs", "link_author": "mercenary_sysadmin", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jblkr1g", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "mercenary_sysadmin", "num_comments": 14, "can_mod_post": false, "send_replies": true, "parent_id": "t1_jblg4qb", "score": 1, "author_fullname": "t2_boter", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "Thanks!", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": true, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Thanks!&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11ldn6f", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/zfs/comments/11ldn6f/how_do_you_install_the_bootloader_on_a/jblkr1g/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/zfs/comments/11ldn6f/how_do_you_install_the_bootloader_on_a/", "name": "t1_jblkr1g", "author_flair_template_id": null, "subreddit_name_prefixed": "r/zfs", "author_flair_text": null, "treatment_tags": [], "created": 1678400701.0, "created_utc": 1678400701.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/zfs/comments/11ldn6f/how_do_you_install_the_bootloader_on_a/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2ruui", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "Pool write slows down read just fine", "mod_reason_by": null, "banned_by": null, "ups": 3, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "zfs", "link_author": "Advanced-King-582", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jbldcus", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "mercenary_sysadmin", "num_comments": 11, "can_mod_post": false, "send_replies": true, "parent_id": "t3_11mnmqe", "score": 3, "author_fullname": "t2_boter", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "&gt; Software stack: Proxmox\n\nYou're getting terrible write performance because Proxmox uses zvols with 8K volblocksize, and you're using rust disks. You're also using a four-wide RAIDz1, which means... a lot of bad things, honestly. Starting with the fact that you can't split 8KiB into three evenly-sized pieces, so what you're actually doing is writing *three*-wide stripes (which means the data is split into two pieces, and the parity is a third), but writing them onto a four disk pool. Which in turn means you're not getting the storage space efficiency you expected out of that four-wide RAIDz1.\n\nYou add all this together, and you get exactly what you're describing: poor and somewhat unpredictable performance. I strongly suspect your reads aren't in as good a shape as you think they are, either: you're probably mostly reading from cache, so not bottlenecking on the actual disks on the read side.\n\nThere's not any single easy \"without a lot of work\" fix for all this. It seems like you managed to get acceptable performance by disabling sync; this implies that an Optane `LOG` vdev would also get you acceptable performance... but it's still not really *right.*\n\nProxmox's 8K volblocksize is pathological for the majority of common workloads. It's appropriate for PostgreSQL database storage, but far smaller than it ought to be for just about anything else. Unfortunately, you can't fix volblocksize in place: you have to create a new zvol of the same size with the desired volblocksize (most workloads will do better with 32K or 64K), then dd or pv the old VM image into the new zvol, then change the storage on the VM itself from the old zvol to the new.\n\nYour drive topology is, I'm sorry to say, just plain ill-suited for the workload. VMs should typically be run from mirrors, not from RAIDz... *especially* on rust drives. If you're going to use RAIDz, you really don't ever want a RAIDz1 vdev wider than three disks... and for VM hosting specifically, I would not recommend RAIDz2 wider than four. (This gets you the same 50% storage efficiency you'd get from two-way mirrors at significantly lower performance, but you *do* get dual parity rather than single out of the deal.)", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;blockquote&gt;\n&lt;p&gt;Software stack: Proxmox&lt;/p&gt;\n&lt;/blockquote&gt;\n\n&lt;p&gt;You&amp;#39;re getting terrible write performance because Proxmox uses zvols with 8K volblocksize, and you&amp;#39;re using rust disks. You&amp;#39;re also using a four-wide RAIDz1, which means... a lot of bad things, honestly. Starting with the fact that you can&amp;#39;t split 8KiB into three evenly-sized pieces, so what you&amp;#39;re actually doing is writing &lt;em&gt;three&lt;/em&gt;-wide stripes (which means the data is split into two pieces, and the parity is a third), but writing them onto a four disk pool. Which in turn means you&amp;#39;re not getting the storage space efficiency you expected out of that four-wide RAIDz1.&lt;/p&gt;\n\n&lt;p&gt;You add all this together, and you get exactly what you&amp;#39;re describing: poor and somewhat unpredictable performance. I strongly suspect your reads aren&amp;#39;t in as good a shape as you think they are, either: you&amp;#39;re probably mostly reading from cache, so not bottlenecking on the actual disks on the read side.&lt;/p&gt;\n\n&lt;p&gt;There&amp;#39;s not any single easy &amp;quot;without a lot of work&amp;quot; fix for all this. It seems like you managed to get acceptable performance by disabling sync; this implies that an Optane &lt;code&gt;LOG&lt;/code&gt; vdev would also get you acceptable performance... but it&amp;#39;s still not really &lt;em&gt;right.&lt;/em&gt;&lt;/p&gt;\n\n&lt;p&gt;Proxmox&amp;#39;s 8K volblocksize is pathological for the majority of common workloads. It&amp;#39;s appropriate for PostgreSQL database storage, but far smaller than it ought to be for just about anything else. Unfortunately, you can&amp;#39;t fix volblocksize in place: you have to create a new zvol of the same size with the desired volblocksize (most workloads will do better with 32K or 64K), then dd or pv the old VM image into the new zvol, then change the storage on the VM itself from the old zvol to the new.&lt;/p&gt;\n\n&lt;p&gt;Your drive topology is, I&amp;#39;m sorry to say, just plain ill-suited for the workload. VMs should typically be run from mirrors, not from RAIDz... &lt;em&gt;especially&lt;/em&gt; on rust drives. If you&amp;#39;re going to use RAIDz, you really don&amp;#39;t ever want a RAIDz1 vdev wider than three disks... and for VM hosting specifically, I would not recommend RAIDz2 wider than four. (This gets you the same 50% storage efficiency you&amp;#39;d get from two-way mirrors at significantly lower performance, but you &lt;em&gt;do&lt;/em&gt; get dual parity rather than single out of the deal.)&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11mnmqe", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/zfs/comments/11mnmqe/pool_write_slows_down_read_just_fine/jbldcus/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/zfs/comments/11mnmqe/pool_write_slows_down_read_just_fine/", "name": "t1_jbldcus", "author_flair_template_id": null, "subreddit_name_prefixed": "r/zfs", "author_flair_text": null, "treatment_tags": [], "created": 1678397764.0, "created_utc": 1678397764.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/zfs/comments/11mnmqe/pool_write_slows_down_read_just_fine/"}}, {"kind": "t1", "data": {"subreddit_id": "t5_2ruui", "approved_at_utc": null, "author_is_blocked": false, "comment_type": null, "link_title": "sandoid --monitor-snaphsots in bash script --&gt; error when run by CRON", "mod_reason_by": null, "banned_by": null, "ups": 3, "num_reports": null, "author_flair_type": "text", "total_awards_received": 0, "subreddit": "zfs", "link_author": "Juggler00", "likes": null, "replies": "", "user_reports": [], "saved": false, "id": "jblau6x", "banned_at_utc": null, "mod_reason_title": null, "gilded": 0, "archived": false, "collapsed_reason_code": null, "no_follow": true, "author": "mercenary_sysadmin", "num_comments": 3, "can_mod_post": false, "send_replies": true, "parent_id": "t1_jbktbby", "score": 3, "author_fullname": "t2_boter", "over_18": false, "report_reasons": null, "removal_reason": null, "approved_by": null, "controversiality": 0, "body": "Yep. That unexpected output is from curl, not from sanoid. You're getting it because, since you're running curl from cron, it knows it's not writing to a terminal, and therefore its output will not get clobbered by a progress bar... so it shows a progress bar.\n\nYou get the same effect if you redirect curl's output to a file. Observe:\n\n    me@elden:/tmp$ curl -m 10 --retry 5 https://hc-ping.com/\n    &lt;html&gt;\n    &lt;head&gt;&lt;title&gt;301 Moved Permanently&lt;/title&gt;&lt;/head&gt;\n    &lt;body&gt;\n    &lt;center&gt;&lt;h1&gt;301 Moved Permanently&lt;/h1&gt;&lt;/center&gt;\n    &lt;hr&gt;&lt;center&gt;nginx&lt;/center&gt;\n    &lt;/body&gt;\n    &lt;/html&gt;\n\nThat's what you expected. But what happens if we pipe the output to a filehandle?\n\n    me@elden:/tmp$ curl -m 10 --retry 5 https://hc-ping.com/ &gt; output.txt\n      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                     Dload  Upload   Total   Spent    Left  Speed\n    100   162  100   162    0     0    341      0 --:--:-- --:--:-- --:--:--   341\n\nLook familiar? :) To keep this from happening, use curl's `-s` (for \"silent\", because those devs were weird and didn't like q for quiet, I guess):\n\n    me@elden:/tmp$ curl -s -m 10 --retry 5 https://hc-ping.com/ &gt; output.txt\n\nNo progress meter!", "edited": false, "top_awarded_type": null, "downs": 0, "author_flair_css_class": null, "is_submitter": false, "collapsed": false, "author_flair_richtext": [], "author_patreon_flair": false, "body_html": "&lt;div class=\"md\"&gt;&lt;p&gt;Yep. That unexpected output is from curl, not from sanoid. You&amp;#39;re getting it because, since you&amp;#39;re running curl from cron, it knows it&amp;#39;s not writing to a terminal, and therefore its output will not get clobbered by a progress bar... so it shows a progress bar.&lt;/p&gt;\n\n&lt;p&gt;You get the same effect if you redirect curl&amp;#39;s output to a file. Observe:&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;me@elden:/tmp$ curl -m 10 --retry 5 https://hc-ping.com/\n&amp;lt;html&amp;gt;\n&amp;lt;head&amp;gt;&amp;lt;title&amp;gt;301 Moved Permanently&amp;lt;/title&amp;gt;&amp;lt;/head&amp;gt;\n&amp;lt;body&amp;gt;\n&amp;lt;center&amp;gt;&amp;lt;h1&amp;gt;301 Moved Permanently&amp;lt;/h1&amp;gt;&amp;lt;/center&amp;gt;\n&amp;lt;hr&amp;gt;&amp;lt;center&amp;gt;nginx&amp;lt;/center&amp;gt;\n&amp;lt;/body&amp;gt;\n&amp;lt;/html&amp;gt;\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;That&amp;#39;s what you expected. But what happens if we pipe the output to a filehandle?&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;me@elden:/tmp$ curl -m 10 --retry 5 https://hc-ping.com/ &amp;gt; output.txt\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   162  100   162    0     0    341      0 --:--:-- --:--:-- --:--:--   341\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;Look familiar? :) To keep this from happening, use curl&amp;#39;s &lt;code&gt;-s&lt;/code&gt; (for &amp;quot;silent&amp;quot;, because those devs were weird and didn&amp;#39;t like q for quiet, I guess):&lt;/p&gt;\n\n&lt;pre&gt;&lt;code&gt;me@elden:/tmp$ curl -s -m 10 --retry 5 https://hc-ping.com/ &amp;gt; output.txt\n&lt;/code&gt;&lt;/pre&gt;\n\n&lt;p&gt;No progress meter!&lt;/p&gt;\n&lt;/div&gt;", "gildings": {}, "collapsed_reason": null, "distinguished": null, "associated_award": null, "stickied": false, "author_premium": false, "can_gild": true, "link_id": "t3_11mwcw4", "unrepliable_reason": null, "author_flair_text_color": null, "score_hidden": false, "permalink": "/r/zfs/comments/11mwcw4/sandoid_monitorsnaphsots_in_bash_script_error/jblau6x/", "subreddit_type": "public", "link_permalink": "https://www.reddit.com/r/zfs/comments/11mwcw4/sandoid_monitorsnaphsots_in_bash_script_error/", "name": "t1_jblau6x", "author_flair_template_id": null, "subreddit_name_prefixed": "r/zfs", "author_flair_text": null, "treatment_tags": [], "created": 1678396799.0, "created_utc": 1678396799.0, "awarders": [], "all_awardings": [], "locked": false, "author_flair_background_color": null, "collapsed_because_crowd_control": null, "mod_reports": [], "quarantine": false, "mod_note": null, "link_url": "https://www.reddit.com/r/zfs/comments/11mwcw4/sandoid_monitorsnaphsots_in_bash_script_error/"}}], "before": null}}